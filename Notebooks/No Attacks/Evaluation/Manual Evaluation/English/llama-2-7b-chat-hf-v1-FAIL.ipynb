{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:36:07.888525Z","iopub.execute_input":"2023-10-08T20:36:07.889147Z","iopub.status.idle":"2023-10-08T20:36:09.065432Z","shell.execute_reply.started":"2023-10-08T20:36:07.889095Z","shell.execute_reply":"2023-10-08T20:36:09.064208Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun Oct  8 20:36:08 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -Uqqq pip --progress-bar off\n!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n!pip install -qqq torch==2.0.1 --progress-bar off\n!pip install -qqq transformers==4.31.0 --progress-bar off\n!pip install -qqq accelerate==0.21.0 --progress-bar off\n!pip install -qqq xformers==0.0.20 --progress-bar off\n!pip install -qqq einops==0.6.1 --progress-bar off\n!pip install -qqq huggingface-hub==0.16.4 --progress-bar off\n!pip install -qqq sentencepiece==0.1.99 --progress-bar off","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:36:09.067232Z","iopub.execute_input":"2023-10-08T20:36:09.067579Z","iopub.status.idle":"2023-10-08T20:40:28.163864Z","shell.execute_reply.started":"2023-10-08T20:36:09.067542Z","shell.execute_reply":"2023-10-08T20:40:28.162573Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom huggingface_hub import notebook_login\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:40:28.166726Z","iopub.execute_input":"2023-10-08T20:40:28.167109Z","iopub.status.idle":"2023-10-08T20:40:42.672039Z","shell.execute_reply.started":"2023-10-08T20:40:28.167072Z","shell.execute_reply":"2023-10-08T20:40:42.671177Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 6.0\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:41:04.059515Z","iopub.execute_input":"2023-10-08T20:41:04.060023Z","iopub.status.idle":"2023-10-08T20:41:04.104667Z","shell.execute_reply.started":"2023-10-08T20:41:04.059989Z","shell.execute_reply":"2023-10-08T20:41:04.103869Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87c7d79004246768ec09eec98e1b8b5"}},"metadata":{}}]},{"cell_type":"code","source":"MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:41:04.106310Z","iopub.execute_input":"2023-10-08T20:41:04.107171Z","iopub.status.idle":"2023-10-08T20:41:04.117857Z","shell.execute_reply.started":"2023-10-08T20:41:04.107139Z","shell.execute_reply":"2023-10-08T20:41:04.116791Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:55:55.521710Z","iopub.execute_input":"2023-10-08T20:55:55.522066Z","iopub.status.idle":"2023-10-08T20:55:56.338725Z","shell.execute_reply.started":"2023-10-08T20:55:55.522041Z","shell.execute_reply":"2023-10-08T20:55:56.337784Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a1e68650624c88b05e2baf92c1d298"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffcaff3cbb32477fa46c5ecc5e5911c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"058efcc300d14572a589697bda3ca058"}},"metadata":{}}]},{"cell_type":"code","source":"model = LlamaForCausalLM.from_pretrained(\n    MODEL_NAME,\n    return_dict=True,\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:44:40.903697Z","iopub.execute_input":"2023-10-08T20:44:40.904240Z","iopub.status.idle":"2023-10-08T20:53:34.688331Z","shell.execute_reply.started":"2023-10-08T20:44:40.904199Z","shell.execute_reply":"2023-10-08T20:53:34.687487Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18d60f27c974c81b15ebe2c11cbfec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a51fa83334e48019e31f1f60e00e799"}},"metadata":{}}]},{"cell_type":"code","source":"generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\ngeneration_config","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:54:35.373862Z","iopub.execute_input":"2023-10-08T20:54:35.374257Z","iopub.status.idle":"2023-10-08T20:54:35.554762Z","shell.execute_reply.started":"2023-10-08T20:54:35.374221Z","shell.execute_reply":"2023-10-08T20:54:35.553801Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9,\n  \"transformers_version\": \"4.31.0\"\n}"},"metadata":{}}]},{"cell_type":"code","source":"model = model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:54:48.385785Z","iopub.execute_input":"2023-10-08T20:54:48.386176Z","iopub.status.idle":"2023-10-08T20:54:48.393400Z","shell.execute_reply.started":"2023-10-08T20:54:48.386119Z","shell.execute_reply":"2023-10-08T20:54:48.391970Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def format_prompt(prompt: str, system_prompt: str) -> str:\n    return f\"\"\"\n{system_prompt}\n{prompt}\n\"\"\".strip()","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:54:48.395265Z","iopub.execute_input":"2023-10-08T20:54:48.395777Z","iopub.status.idle":"2023-10-08T20:54:48.405004Z","shell.execute_reply.started":"2023-10-08T20:54:48.395746Z","shell.execute_reply":"2023-10-08T20:54:48.404139Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def generate_response(prompt: str, max_new_tokens: int = 128) -> str:\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            **encoding,\n            max_new_tokens=max_new_tokens,\n            temperature=0.0001,\n            generation_config=generation_config,\n        )\n    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n    return tokenizer.decode(answer_tokens[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:54:48.406616Z","iopub.execute_input":"2023-10-08T20:54:48.407740Z","iopub.status.idle":"2023-10-08T20:54:48.415415Z","shell.execute_reply.started":"2023-10-08T20:54:48.407711Z","shell.execute_reply":"2023-10-08T20:54:48.414498Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# text = \"\"\"\n# What is twitter\n# \"\"\"\n\nprompt = f\"\"\"\nWrite detailed notes based on this transcript of a lecture. \nUse markdown. \nUse several layers of nested headings and bold and italics but don't use any headings past heading 3. \nYou should also include a detailed table of content.\nDon't include any sponsored ads\n\n6\n00:00:10,980 --> 00:00:15,660\n today we're going to talk\n\n7\n00:00:13,080 --> 00:00:17,899\nabout Transformer neural networks and\n\n8\n00:00:15,660 --> 00:00:21,359\nthey're going to be clearly explained\n\n12\n00:00:25,679 --> 00:00:32,159\nright now people are going bonkers about\n\n13\n00:00:28,739 --> 00:00:35,100\nsomething called chat GPT for example\n\n14\n00:00:32,159 --> 00:00:38,039\nour friend statsquatch might type\n\n15\n00:00:35,100 --> 00:00:42,558\nsomething into chat GPT like\n\n16\n00:00:38,039 --> 00:00:44,479\nright and awesome song in the style of\n\n17\n00:00:42,558 --> 00:00:47,820\nstatquest\n\n18\n00:00:44,479 --> 00:00:49,738\ntranslation it's done with a transform\n\n19\n00:00:47,820 --> 00:00:52,320\nER\n\n20\n00:00:49,738 --> 00:00:55,919\nanyway there's a lot to be said about\n\n21\n00:00:52,320 --> 00:00:57,539\nhow chat GPT works but fundamentally it\n\n22\n00:00:55,920 --> 00:00:58,980\nis based on something called a\n\n23\n00:00:57,539 --> 00:01:01,739\nTransformer\n\n24\n00:00:58,979 --> 00:01:04,259\nso in this stat Quest we're going to\n\n25\n00:01:01,738 --> 00:01:06,118\nshow you how a Transformer works one\n\n26\n00:01:04,260 --> 00:01:08,880\nstep at a time\n\n27\n00:01:06,118 --> 00:01:10,978\nspecifically we're going to focus on how\n\n28\n00:01:08,879 --> 00:01:13,039\na Transformer neural network can\n\n29\n00:01:10,978 --> 00:01:17,700\ntranslate a simple English sentence\n\n30\n00:01:13,040 --> 00:01:20,640\nlet's go into Spanish vamos\n\n31\n00:01:17,700 --> 00:01:22,618\nnow since a Transformer is a type of\n\n32\n00:01:20,640 --> 00:01:24,840\nneural network and neural networks\n\n33\n00:01:22,618 --> 00:01:25,859\nusually only have numbers for input\n\n34\n00:01:24,840 --> 00:01:28,439\nvalues\n\n35\n00:01:25,859 --> 00:01:30,500\nthe first thing we need to do is find a\n\n36\n00:01:28,438 --> 00:01:32,819\nway to turn the input and output words\n\n37\n00:01:30,500 --> 00:01:35,219\ninto numbers\n\n38\n00:01:32,819 --> 00:01:38,158\nthere are a lot of ways to convert words\n\n39\n00:01:35,219 --> 00:01:40,200\ninto numbers but for neural networks one\n\n40\n00:01:38,159 --> 00:01:43,200\nof the most commonly used methods is\n\n41\n00:01:40,200 --> 00:01:45,180\ncalled word embedding the main idea of\n\n42\n00:01:43,200 --> 00:01:48,478\nword embedding is to use a relatively\n\n43\n00:01:45,180 --> 00:01:50,700\nsimple neural network that has one input\n\n44\n00:01:48,478 --> 00:01:54,000\nfor every word and symbol in the\n\n45\n00:01:50,700 --> 00:01:56,700\nvocabulary that you want to use in this\n\n46\n00:01:54,000 --> 00:01:59,099\ncase we have a super simple vocabulary\n\n47\n00:01:56,700 --> 00:02:02,100\nthat allows us to input short phrases\n\n48\n00:01:59,099 --> 00:02:05,158\nlike let's go and to go\n\n49\n00:02:02,099 --> 00:02:08,159\nand we have an input for this symbol EOS\n\n50\n00:02:05,159 --> 00:02:11,219\nwhich stands for end of sentence or end\n\n51\n00:02:08,159 --> 00:02:13,859\nof sequence because the vocabulary can\n\n52\n00:02:11,219 --> 00:02:17,340\nbe a mix of words word fragments and\n\n53\n00:02:13,860 --> 00:02:19,379\nsymbols we call each input a token\n\n54\n00:02:17,340 --> 00:02:21,239\nthe inputs are then connected to\n\n55\n00:02:19,378 --> 00:02:23,639\nsomething called an activation function\n\n56\n00:02:21,239 --> 00:02:25,439\nand in this example we have two\n\n57\n00:02:23,639 --> 00:02:28,019\nactivation functions\n\n58\n00:02:25,439 --> 00:02:30,840\nand each connection multiplies the input\n\n59\n00:02:28,020 --> 00:02:33,659\nvalue by something called a weight\n\n60\n00:02:30,840 --> 00:02:34,560\nhey Josh where do these numbers come\n\n61\n00:02:33,659 --> 00:02:37,020\nfrom\n\n62\n00:02:34,560 --> 00:02:38,759\ngreat question Squatch and we'll answer\n\n63\n00:02:37,020 --> 00:02:41,280\nit in just a bit\n\n64\n00:02:38,759 --> 00:02:43,859\nfor now let's just see how we convert\n\n65\n00:02:41,280 --> 00:02:46,500\nthe word let's into numbers\n\n66\n00:02:43,860 --> 00:02:49,860\nfirst we put a 1 into the input for\n\n67\n00:02:46,500 --> 00:02:51,479\nlet's and then put zeros into all of the\n\n68\n00:02:49,860 --> 00:02:53,760\nother inputs\n\n69\n00:02:51,479 --> 00:02:55,560\nnow we multiply the inputs by their\n\n70\n00:02:53,759 --> 00:02:57,359\nweights on the connections to the\n\n71\n00:02:55,560 --> 00:03:00,420\nactivation functions\n\n72\n00:02:57,360 --> 00:03:05,280\nfor example the input for let's is one\n\n73\n00:03:00,419 --> 00:03:07,859\nso we multiply 1.87 by 1 to get 1.87\n\n74\n00:03:05,280 --> 00:03:12,860\ngoing to the activation function on the\n\n75\n00:03:07,860 --> 00:03:16,319\nleft and we multiply 0.09 by 1 to get\n\n76\n00:03:12,860 --> 00:03:17,459\n0.09 going to the activation function on\n\n77\n00:03:16,318 --> 00:03:20,219\nthe right\n\n78\n00:03:17,459 --> 00:03:22,680\nin contrast if the input value for the\n\n79\n00:03:20,219 --> 00:03:27,060\nword 2 is 0\n\n80\n00:03:22,680 --> 00:03:29,280\nthen we multiply negative 1.45 by 0 to\n\n81\n00:03:27,060 --> 00:03:30,719\nget 0 going to the activation function\n\n82\n00:03:29,280 --> 00:03:34,919\non the left\n\n83\n00:03:30,719 --> 00:03:37,979\nand we multiply 1.50 by 0 to get 0 going\n\n84\n00:03:34,919 --> 00:03:40,738\nto the activation function on the right\n\n85\n00:03:37,979 --> 00:03:43,199\nin other words when an input value is 0\n\n86\n00:03:40,739 --> 00:03:46,920\nthen it only sends zeros to the\n\n87\n00:03:43,199 --> 00:03:50,219\nactivation functions and that means to\n\n88\n00:03:46,919 --> 00:03:52,919\ngo and the EOS symbol all just send\n\n89\n00:03:50,219 --> 00:03:55,620\nzeros to the activation functions\n\n90\n00:03:52,919 --> 00:03:57,598\nand only the weight values for let's end\n\n91\n00:03:55,620 --> 00:04:00,299\nup at the activation functions because\n\n92\n00:03:57,598 --> 00:04:04,018\nits input value is 1.\n\n93\n00:04:00,299 --> 00:04:06,540\nso in this case 1.87 goes to the\n\n94\n00:04:04,019 --> 00:04:10,140\nactivation function on the left\n\n95\n00:04:06,539 --> 00:04:12,000\nand 0.09 goes to the activation function\n\n96\n00:04:10,139 --> 00:04:14,818\non the right\n\n97\n00:04:12,000 --> 00:04:17,279\nin this example the activation functions\n\n98\n00:04:14,818 --> 00:04:19,620\nthemselves are just identity functions\n\n99\n00:04:17,279 --> 00:04:21,599\nmeaning the output values are the same\n\n100\n00:04:19,620 --> 00:04:24,478\nas the input values\n\n101\n00:04:21,600 --> 00:04:26,639\nin other words if the input value or\n\n102\n00:04:24,478 --> 00:04:30,779\nx-axis coordinate for the activation\n\n103\n00:04:26,639 --> 00:04:33,720\nfunction on the left is 1.87 then the\n\n104\n00:04:30,779 --> 00:04:36,059\noutput value the y-axis coordinate will\n\n105\n00:04:33,720 --> 00:04:38,400\nalso be 1.87\n\n106\n00:04:36,060 --> 00:04:42,360\nlikewise because the input to the\n\n107\n00:04:38,399 --> 00:04:46,019\nactivation function on the right is 0.09\n\n108\n00:04:42,360 --> 00:04:50,879\nthe output is also 0.09\n\n109\n00:04:46,019 --> 00:04:53,159\nthus these output values 1.87 and 0.09\n\n110\n00:04:50,879 --> 00:04:55,639\nare the numbers that represent the word\n\n111\n00:04:53,160 --> 00:04:58,259\nleads bam\n\n112\n00:04:55,639 --> 00:05:00,300\nlikewise if we want to convert the word\n\n113\n00:04:58,259 --> 00:05:04,560\ngo into numbers\n\n114\n00:05:00,300 --> 00:05:07,379\nwe set the input value for go to 1. and\n\n115\n00:05:04,560 --> 00:05:11,000\nall of the other inputs to zero\n\n116\n00:05:07,379 --> 00:05:13,978\nand we end up with negative 0.78 and\n\n117\n00:05:11,000 --> 00:05:15,660\n0.27 as the numbers that represent the\n\n118\n00:05:13,978 --> 00:05:18,180\nword go\n\n119\n00:05:15,660 --> 00:05:21,300\nand that is how we use word embedding to\n\n120\n00:05:18,180 --> 00:05:24,300\nconvert our input phrase let's go into\n\n121\n00:05:21,300 --> 00:05:26,939\nnumbers bam\n\n122\n00:05:24,300 --> 00:05:28,860\nnote there is a lot more to say about\n\n123\n00:05:26,939 --> 00:05:30,600\nword embedding so if you're interested\n\n124\n00:05:28,860 --> 00:05:33,538\ncheck out the quest\n\n125\n00:05:30,600 --> 00:05:35,340\nalso note before we move on I want to\n\n126\n00:05:33,538 --> 00:05:38,038\npoint out two things\n\n127\n00:05:35,339 --> 00:05:41,159\nfirst we reuse the same word embedding\n\n128\n00:05:38,038 --> 00:05:43,319\nNetwork for each input word or symbol\n\n129\n00:05:41,160 --> 00:05:45,000\nin other words the weights in the\n\n130\n00:05:43,319 --> 00:05:47,639\nnetwork for let's\n\n131\n00:05:45,000 --> 00:05:49,500\nare the exact same as the weights in the\n\n132\n00:05:47,639 --> 00:05:51,960\nnetwork for go\n\n133\n00:05:49,500 --> 00:05:54,720\nthis means that regardless of how long\n\n134\n00:05:51,959 --> 00:05:56,579\nthe input sentence is we just copy and\n\n135\n00:05:54,720 --> 00:05:59,280\nuse the exact same word embedding\n\n136\n00:05:56,579 --> 00:06:01,740\nNetwork for each word or symbol\n\n137\n00:05:59,279 --> 00:06:04,739\nand this gives us flexibility to handle\n\n138\n00:06:01,740 --> 00:06:06,840\ninput sentences with different lengths\n\n139\n00:06:04,740 --> 00:06:08,939\nthe second thing I want to mention is\n\n140\n00:06:06,839 --> 00:06:10,319\nthat all of these weights and all of the\n\n141\n00:06:08,939 --> 00:06:12,959\nother weights we're going to talk about\n\n142\n00:06:10,319 --> 00:06:15,418\nin this Quest are determined using\n\n143\n00:06:12,959 --> 00:06:17,698\nsomething called back propagation\n\n144\n00:06:15,418 --> 00:06:20,939\nto get a sense of what back propagation\n\n145\n00:06:17,699 --> 00:06:23,400\ndoes let's imagine we had this data and\n\n146\n00:06:20,939 --> 00:06:25,800\nwe wanted to fit a line to it\n\n147\n00:06:23,399 --> 00:06:28,258\nback propagation would start with a line\n\n148\n00:06:25,800 --> 00:06:31,079\nthat has a random value for the y-axis\n\n149\n00:06:28,259 --> 00:06:31,979\nintercept and a random value for the\n\n150\n00:06:31,079 --> 00:06:35,339\nslope\n\n151\n00:06:31,978 --> 00:06:37,680\nand then using an iterative process back\n\n152\n00:06:35,339 --> 00:06:40,560\npropagation would change the y-axis\n\n153\n00:06:37,680 --> 00:06:43,939\nintercept and slope one step at a time\n\n154\n00:06:40,560 --> 00:06:46,500\nuntil it found the optimal values\n\n155\n00:06:43,939 --> 00:06:49,079\nlikewise in the context of neural\n\n156\n00:06:46,500 --> 00:06:50,399\nnetworks each weight starts out as a\n\n157\n00:06:49,079 --> 00:06:53,038\nrandom number\n\n158\n00:06:50,399 --> 00:06:55,079\nbut when we train the Transformer with\n\n159\n00:06:53,038 --> 00:06:56,818\nEnglish phrases and known Spanish\n\n160\n00:06:55,079 --> 00:06:59,639\ntranslations\n\n161\n00:06:56,819 --> 00:07:02,520\nback propagation optimizes these values\n\n162\n00:06:59,639 --> 00:07:04,079\none step at a time and results in these\n\n163\n00:07:02,519 --> 00:07:07,318\nfinal weights\n\n164\n00:07:04,079 --> 00:07:09,478\nalso just to be clear the process of\n\n165\n00:07:07,319 --> 00:07:11,819\noptimizing the weights is also called\n\n166\n00:07:09,478 --> 00:07:14,339\ntraining bam\n\n167\n00:07:11,819 --> 00:07:16,860\nnote there is a lot more to be said\n\n168\n00:07:14,339 --> 00:07:18,418\nabout training and back propagation so\n\n169\n00:07:16,860 --> 00:07:19,560\nif you're interested check out the\n\n170\n00:07:18,418 --> 00:07:22,019\nquests\n\n171\n00:07:19,560 --> 00:07:24,538\nnow because the word embedding networks\n\n172\n00:07:22,019 --> 00:07:26,279\nare taking up the whole screen let's\n\n173\n00:07:24,538 --> 00:07:29,399\nshrink them down and put them in the\n\n174\n00:07:26,279 --> 00:07:32,279\ncorner okay and now that we know how to\n\n175\n00:07:29,399 --> 00:07:34,198\nconvert words into numbers let's talk\n\n176\n00:07:32,279 --> 00:07:37,978\nabout word order\n\n177\n00:07:34,199 --> 00:07:43,080\nfor example if Norm said Squatch eats\n\n178\n00:07:37,978 --> 00:07:47,038\npizza then squash might say yum\n\n179\n00:07:43,079 --> 00:07:52,019\nin contrast if Norm said Pizza eats\n\n180\n00:07:47,038 --> 00:07:54,180\nsquash then squash might say yikes\n\n181\n00:07:52,019 --> 00:07:56,399\nso these two phrases\n\n182\n00:07:54,180 --> 00:08:00,060\nSquatch eats Pizza\n\n183\n00:07:56,399 --> 00:08:03,299\nand pizza eats squash use the exact same\n\n184\n00:08:00,060 --> 00:08:06,300\nwords but have very different meanings\n\n185\n00:08:03,300 --> 00:08:09,060\nso keeping track of word order is super\n\n186\n00:08:06,300 --> 00:08:11,098\nimportant so let's talk about positional\n\n187\n00:08:09,060 --> 00:08:13,379\nencoding which is a technique that\n\n188\n00:08:11,098 --> 00:08:14,459\nTransformers use to keep track of word\n\n189\n00:08:13,379 --> 00:08:16,500\norder\n\n190\n00:08:14,459 --> 00:08:18,839\nwe'll start by showing how to add\n\n191\n00:08:16,500 --> 00:08:21,478\npositional encoding to the first phrase\n\n192\n00:08:18,839 --> 00:08:23,939\nSquatch eats Pizza\n\n193\n00:08:21,478 --> 00:08:26,098\nnote there are a bunch of ways to do\n\n194\n00:08:23,939 --> 00:08:28,680\npositional encoding but we're just going\n\n195\n00:08:26,098 --> 00:08:31,019\nto talk about one popular method\n\n196\n00:08:28,680 --> 00:08:34,019\nthat said the first thing we do is\n\n197\n00:08:31,019 --> 00:08:36,478\nconvert the words squash eats pizza into\n\n198\n00:08:34,019 --> 00:08:38,519\nnumbers using word embedding\n\n199\n00:08:36,479 --> 00:08:40,979\nin this example we've got a new\n\n200\n00:08:38,519 --> 00:08:43,379\nvocabulary and we're creating four word\n\n201\n00:08:40,979 --> 00:08:46,379\nembedding values per word\n\n202\n00:08:43,379 --> 00:08:48,838\nhowever in practice people often create\n\n203\n00:08:46,379 --> 00:08:50,820\nhundreds or even thousands of embedding\n\n204\n00:08:48,839 --> 00:08:52,920\nvalues per word\n\n205\n00:08:50,820 --> 00:08:54,959\nnow we add a set of numbers that\n\n206\n00:08:52,919 --> 00:08:58,019\ncorrespond to word order to the\n\n207\n00:08:54,958 --> 00:09:00,479\nembedding values for each word hey Josh\n\n208\n00:08:58,019 --> 00:09:02,459\nwhere do the numbers that correspond to\n\n209\n00:09:00,480 --> 00:09:04,920\nword order come from\n\n210\n00:09:02,458 --> 00:09:07,079\nin this case the numbers that represent\n\n211\n00:09:04,919 --> 00:09:10,439\nthe word order come from a sequence of\n\n212\n00:09:07,080 --> 00:09:12,839\nalternating sine and cosine squiggles\n\n213\n00:09:10,440 --> 00:09:15,660\neach squiggle gives a specific position\n\n214\n00:09:12,839 --> 00:09:18,720\nvalues for each word's embeddings\n\n215\n00:09:15,659 --> 00:09:20,939\nfor example the y-axis values on the\n\n216\n00:09:18,720 --> 00:09:23,278\ngreen squiggle give us position encoding\n\n217\n00:09:20,940 --> 00:09:24,380\nvalues for the first embeddings for each\n\n218\n00:09:23,278 --> 00:09:27,299\nword\n\n219\n00:09:24,379 --> 00:09:29,939\nspecifically for the first word which\n\n220\n00:09:27,299 --> 00:09:32,579\nhas an x-axis coordinate all the way to\n\n221\n00:09:29,940 --> 00:09:34,500\nthe left of the green squiggle the\n\n222\n00:09:32,580 --> 00:09:37,980\nposition value for the first embedding\n\n223\n00:09:34,500 --> 00:09:39,839\nis the y-axis coordinate zero\n\n224\n00:09:37,980 --> 00:09:42,120\nthe position value for the second\n\n225\n00:09:39,839 --> 00:09:44,760\nembedding comes from the orange squiggle\n\n226\n00:09:42,120 --> 00:09:46,919\nand the y-axis coordinate on the orange\n\n227\n00:09:44,759 --> 00:09:48,899\nsquiggle that corresponds to the first\n\n228\n00:09:46,919 --> 00:09:51,599\nword is one\n\n229\n00:09:48,899 --> 00:09:53,720\nlikewise the blue squiggle which is more\n\n230\n00:09:51,600 --> 00:09:56,399\nspread out than the first two squiggles\n\n231\n00:09:53,720 --> 00:09:58,379\ngives us the position value for the\n\n232\n00:09:56,399 --> 00:10:00,539\nthird embedding value which for the\n\n233\n00:09:58,379 --> 00:10:03,120\nfirst word is zero\n\n234\n00:10:00,539 --> 00:10:05,639\nlastly the red squiggle gives us the\n\n235\n00:10:03,120 --> 00:10:09,120\nposition value for the fourth embedding\n\n236\n00:10:05,639 --> 00:10:11,519\nwhich for the first word is one\n\n237\n00:10:09,120 --> 00:10:14,039\nthus the position values for the first\n\n238\n00:10:11,519 --> 00:10:16,259\nword come from the corresponding y-axis\n\n239\n00:10:14,039 --> 00:10:18,899\ncoordinates on the squiggles\n\n240\n00:10:16,259 --> 00:10:21,899\nnow to get the position values for the\n\n241\n00:10:18,899 --> 00:10:23,580\nsecond word we simply use the y-axis\n\n242\n00:10:21,899 --> 00:10:25,980\ncoordinates on the squiggles that\n\n243\n00:10:23,580 --> 00:10:27,660\ncorrespond to the x-axis coordinate for\n\n244\n00:10:25,980 --> 00:10:30,360\nthe second word\n\n245\n00:10:27,659 --> 00:10:33,179\nlastly to get the position values for\n\n246\n00:10:30,360 --> 00:10:34,740\nthe third word we use the y-axis\n\n247\n00:10:33,179 --> 00:10:37,019\ncoordinates on the squiggles that\n\n248\n00:10:34,740 --> 00:10:38,700\ncorrespond to the x-axis coordinate for\n\n249\n00:10:37,019 --> 00:10:41,278\nthe third word\n\n250\n00:10:38,700 --> 00:10:43,620\nnote because the sine and cosine\n\n251\n00:10:41,278 --> 00:10:45,480\nsquiggles are repetitive it's possible\n\n252\n00:10:43,620 --> 00:10:48,778\nthat two words might get the same\n\n253\n00:10:45,480 --> 00:10:51,180\nposition or y-axis values\n\n254\n00:10:48,778 --> 00:10:54,120\nfor example the second and third words\n\n255\n00:10:51,179 --> 00:10:55,739\nboth got negative 0.9 for the first\n\n256\n00:10:54,120 --> 00:10:58,500\nposition value\n\n257\n00:10:55,740 --> 00:11:01,259\nhowever because the squiggles get wider\n\n258\n00:10:58,500 --> 00:11:03,480\nfor larger embedding positions and the\n\n259\n00:11:01,259 --> 00:11:05,338\nmore embedding values we have then the\n\n260\n00:11:03,480 --> 00:11:08,159\nwider the squiggles get\n\n261\n00:11:05,339 --> 00:11:10,440\nthen even with a repeat value here and\n\n262\n00:11:08,159 --> 00:11:13,338\nthere we end up with a unique sequence\n\n263\n00:11:10,440 --> 00:11:16,500\nof position values for each word\n\n264\n00:11:13,339 --> 00:11:19,079\nthus each input word ends up with a\n\n265\n00:11:16,500 --> 00:11:21,299\nunique sequence of position values\n\n266\n00:11:19,078 --> 00:11:23,399\nnow all we have to do is add the\n\n267\n00:11:21,299 --> 00:11:25,979\nposition values to the embedding values\n\n268\n00:11:23,399 --> 00:11:28,200\nand we end up with the word embeddings\n\n269\n00:11:25,980 --> 00:11:31,879\nplus positional encoding for the whole\n\n270\n00:11:28,200 --> 00:11:33,660\nsentence Squatch eats Pizza\n\n271\n00:11:31,879 --> 00:11:36,299\nyum\n\n272\n00:11:33,659 --> 00:11:40,679\nnote if we reverse the order of the\n\n273\n00:11:36,299 --> 00:11:42,599\ninput words to be Pizza eats squash then\n\n274\n00:11:40,679 --> 00:11:45,359\nthe embeddings for the first and third\n\n275\n00:11:42,600 --> 00:11:47,700\nwords get swapped but the positional\n\n276\n00:11:45,360 --> 00:11:51,000\nvalues for the first second and third\n\n277\n00:11:47,700 --> 00:11:53,519\nword stay the same and when we add the\n\n278\n00:11:51,000 --> 00:11:55,919\npositional values to the embeddings\n\n279\n00:11:53,519 --> 00:11:58,980\nwe end up with new positional encoding\n\n280\n00:11:55,919 --> 00:12:01,439\nfor the first and third words and the\n\n281\n00:11:58,980 --> 00:12:02,579\nsecond word since it didn't move stays\n\n282\n00:12:01,440 --> 00:12:05,160\nthe same\n\n283\n00:12:02,578 --> 00:12:07,139\nthus positional encoding allows a\n\n284\n00:12:05,159 --> 00:12:10,139\nTransformer to keep track of word order\n\n285\n00:12:07,139 --> 00:12:10,139\nbam\n\n286\n00:12:10,200 --> 00:12:14,220\nnow let's go back to our simple example\n\n287\n00:12:12,419 --> 00:12:16,919\nwhere we are just trying to translate\n\n288\n00:12:14,220 --> 00:12:19,019\nthe English sentence let's go\n\n289\n00:12:16,919 --> 00:12:20,219\nand add position values to the word\n\n290\n00:12:19,019 --> 00:12:22,320\nembeddings\n\n291\n00:12:20,220 --> 00:12:25,860\nthe first embedding for the first word\n\n292\n00:12:22,320 --> 00:12:28,920\nLet's gets zero and the second embedding\n\n293\n00:12:25,860 --> 00:12:33,120\ngets one and the first embedding for the\n\n294\n00:12:28,919 --> 00:12:36,240\nsecond word go gets a negative 0.9 and\n\n295\n00:12:33,120 --> 00:12:38,578\nthe second embedding gets 0.4\n\n296\n00:12:36,240 --> 00:12:42,778\nand now we just do the math to get the\n\n297\n00:12:38,578 --> 00:12:45,179\npositional encoding for both words bam\n\n298\n00:12:42,778 --> 00:12:47,458\nnow because we're going to need all the\n\n299\n00:12:45,179 --> 00:12:50,458\nspace we can get let's consolidate the\n\n300\n00:12:47,458 --> 00:12:52,859\nmath in the diagram and let the sine and\n\n301\n00:12:50,458 --> 00:12:54,778\ncosine and plus symbols represent the\n\n302\n00:12:52,860 --> 00:12:56,940\npositional encoding\n\n303\n00:12:54,778 --> 00:12:59,700\nnow that we know how to keep track of\n\n304\n00:12:56,940 --> 00:13:01,920\neach word's position let's talk about\n\n305\n00:12:59,700 --> 00:13:04,620\nhow a Transformer keeps track of the\n\n306\n00:13:01,919 --> 00:13:07,379\nrelationships among words\n\n307\n00:13:04,620 --> 00:13:10,919\nfor example if the input sentence was\n\n308\n00:13:07,379 --> 00:13:15,000\nthis the pizza came out of the oven and\n\n309\n00:13:10,919 --> 00:13:18,659\nit tasted good then this word it could\n\n310\n00:13:15,000 --> 00:13:22,139\nrefer to pizza or potentially it could\n\n311\n00:13:18,659 --> 00:13:24,719\nrefer to the word oven Josh I've heard\n\n312\n00:13:22,139 --> 00:13:28,200\nof good tasting pizza but never a good\n\n313\n00:13:24,720 --> 00:13:29,940\ntasting oven I know Squatch that's why\n\n314\n00:13:28,200 --> 00:13:32,579\nit's important that the Transformer\n\n315\n00:13:29,940 --> 00:13:35,700\ncorrectly Associates the word it with\n\n316\n00:13:32,578 --> 00:13:37,379\npizza the good news is that Transformers\n\n317\n00:13:35,700 --> 00:13:39,540\nhave something called self-attention\n\n318\n00:13:37,379 --> 00:13:41,820\nwhich is a mechanism to correctly\n\n319\n00:13:39,539 --> 00:13:43,019\nassociate the word ID with the word\n\n320\n00:13:41,820 --> 00:13:46,260\nPizza\n\n321\n00:13:43,019 --> 00:13:48,778\nin general terms self-attention works by\n\n322\n00:13:46,259 --> 00:13:51,000\nseeing how similar each word is to all\n\n323\n00:13:48,778 --> 00:13:54,059\nof the words in the sentence including\n\n324\n00:13:51,000 --> 00:13:56,278\nitself for example self-attention\n\n325\n00:13:54,059 --> 00:13:59,159\ncalculates the similarity between the\n\n326\n00:13:56,278 --> 00:14:02,458\nfirst word the and all of the words in\n\n327\n00:13:59,159 --> 00:14:04,620\nthe sentence including itself\n\n328\n00:14:02,458 --> 00:14:06,599\nand self-attention calculates these\n\n329\n00:14:04,620 --> 00:14:07,980\nsimilarities for every word in the\n\n330\n00:14:06,600 --> 00:14:10,139\nsentence\n\n331\n00:14:07,980 --> 00:14:12,300\nonce the similarities are calculated\n\n332\n00:14:10,139 --> 00:14:15,120\nthey are used to determine how the\n\n333\n00:14:12,299 --> 00:14:17,519\nTransformer encodes each word\n\n334\n00:14:15,120 --> 00:14:19,980\nfor example if you looked at a lot of\n\n335\n00:14:17,519 --> 00:14:22,379\nsentences about pizza and the word ID\n\n336\n00:14:19,980 --> 00:14:24,060\nwas more commonly associated with pizza\n\n337\n00:14:22,379 --> 00:14:26,820\nthan oven\n\n338\n00:14:24,059 --> 00:14:28,919\nthen the similarity score for pizza will\n\n339\n00:14:26,820 --> 00:14:31,019\ncause it to have a larger impact on how\n\n340\n00:14:28,919 --> 00:14:32,278\nthe word ID is encoded by the\n\n341\n00:14:31,019 --> 00:14:33,839\nTransformer\n\n342\n00:14:32,278 --> 00:14:36,120\nbam\n\n343\n00:14:33,839 --> 00:14:38,399\nand now that we know the main ideas of\n\n344\n00:14:36,120 --> 00:14:39,659\nhow self-attention Works let's look at\n\n345\n00:14:38,399 --> 00:14:41,759\nthe details\n\n346\n00:14:39,659 --> 00:14:43,860\nso let's go back to our simple example\n\n347\n00:14:41,759 --> 00:14:47,399\nwhere we had just added positional\n\n348\n00:14:43,860 --> 00:14:49,680\nencoding to the words let's and go\n\n349\n00:14:47,399 --> 00:14:51,659\nthe first thing we do is multiply the\n\n350\n00:14:49,679 --> 00:14:54,838\nposition encoded values for the word\n\n351\n00:14:51,659 --> 00:14:57,059\nlet's by a pair of weights and we add\n\n352\n00:14:54,839 --> 00:14:58,740\nthose products together to get Negative\n\n353\n00:14:57,059 --> 00:15:00,719\n1.0\n\n354\n00:14:58,740 --> 00:15:04,680\nthen we do the same thing with a\n\n355\n00:15:00,720 --> 00:15:07,199\ndifferent pair of weights to get 3.7\n\n356\n00:15:04,679 --> 00:15:09,299\nwe do this twice because we started out\n\n357\n00:15:07,198 --> 00:15:12,479\nwith two position encoded values that\n\n358\n00:15:09,299 --> 00:15:15,240\nrepresent the word leads and after doing\n\n359\n00:15:12,480 --> 00:15:17,940\nthe math two times we still have two\n\n360\n00:15:15,240 --> 00:15:20,339\nvalues representing the word leads\n\n361\n00:15:17,940 --> 00:15:22,920\nJosh I don't get it\n\n362\n00:15:20,339 --> 00:15:25,620\nif we want two values to represent let's\n\n363\n00:15:22,919 --> 00:15:26,819\nwhy don't we just use the two values we\n\n364\n00:15:25,620 --> 00:15:29,100\nstarted with\n\n365\n00:15:26,820 --> 00:15:33,360\nthat's a great question Squatch and\n\n366\n00:15:29,100 --> 00:15:36,240\nwe'll answer it in a little bit grr\n\n367\n00:15:33,360 --> 00:15:38,519\nanyway for now just know that we have\n\n368\n00:15:36,240 --> 00:15:40,500\nthese two new values to represent the\n\n369\n00:15:38,519 --> 00:15:43,198\nword let's and in Transformer\n\n370\n00:15:40,500 --> 00:15:45,778\nterminology we call them query values\n\n371\n00:15:43,198 --> 00:15:48,599\nand now that we have query values for\n\n372\n00:15:45,778 --> 00:15:50,939\nthe word let's use them to calculate the\n\n373\n00:15:48,600 --> 00:15:52,079\nsimilarity between itself and the word\n\n374\n00:15:50,940 --> 00:15:54,540\ngo\n\n375\n00:15:52,078 --> 00:15:57,239\nand we do this by creating two new\n\n376\n00:15:54,539 --> 00:15:59,338\nvalues just like we did for the query to\n\n377\n00:15:57,240 --> 00:16:01,500\nrepresent the word let's\n\n378\n00:15:59,339 --> 00:16:03,720\nand we create two new values to\n\n379\n00:16:01,500 --> 00:16:06,600\nrepresent the word go\n\n380\n00:16:03,720 --> 00:16:07,860\nboth sets of new values are called key\n\n381\n00:16:06,600 --> 00:16:09,600\nvalues\n\n382\n00:16:07,860 --> 00:16:13,019\nand we use them to calculate\n\n383\n00:16:09,600 --> 00:16:15,180\nsimilarities with the query for let's\n\n384\n00:16:13,019 --> 00:16:17,759\none way to calculate similarities\n\n385\n00:16:15,179 --> 00:16:20,399\nbetween the query and the keys is to\n\n386\n00:16:17,759 --> 00:16:22,860\ncalculate something called a DOT product\n\n387\n00:16:20,399 --> 00:16:25,198\nfor example in order to calculate the\n\n388\n00:16:22,860 --> 00:16:27,240\ndot product similarity between the query\n\n389\n00:16:25,198 --> 00:16:29,698\nand key for let's\n\n390\n00:16:27,240 --> 00:16:30,959\nwe simply multiply each pair of numbers\n\n391\n00:16:29,698 --> 00:16:34,859\ntogether\n\n392\n00:16:30,958 --> 00:16:37,198\nand then add the products to get 11.7\n\n393\n00:16:34,860 --> 00:16:39,300\nlikewise we can calculate the dot\n\n394\n00:16:37,198 --> 00:16:41,819\nproduct similarity between the query for\n\n395\n00:16:39,299 --> 00:16:43,799\nlet's and the key for go\n\n396\n00:16:41,820 --> 00:16:45,060\nby multiplying the pairs of numbers\n\n397\n00:16:43,799 --> 00:16:47,399\ntogether\n\n398\n00:16:45,059 --> 00:16:49,198\nand adding the products to get negative\n\n399\n00:16:47,399 --> 00:16:51,659\n2.6\n\n400\n00:16:49,198 --> 00:16:54,258\nthe relatively large similarity value\n\n401\n00:16:51,659 --> 00:16:57,838\nfor let's relative to itself\n\n402\n00:16:54,259 --> 00:17:00,659\n11.7 compared to the relatively small\n\n403\n00:16:57,839 --> 00:17:03,060\nvalue for lets relative to the word go\n\n404\n00:17:00,659 --> 00:17:05,699\nnegative 2.6\n\n405\n00:17:03,059 --> 00:17:09,058\ntells us that let's is much more similar\n\n406\n00:17:05,699 --> 00:17:11,519\nto itself than it is to the word go\n\n407\n00:17:09,058 --> 00:17:13,980\nthat said if you remember the example\n\n408\n00:17:11,519 --> 00:17:15,418\nwhere the word it could relate to pizza\n\n409\n00:17:13,980 --> 00:17:17,880\nor oven\n\n410\n00:17:15,419 --> 00:17:20,280\nthe word it should have a relatively\n\n411\n00:17:17,880 --> 00:17:22,980\nlarge similarity value with respect to\n\n412\n00:17:20,279 --> 00:17:25,019\nthe word Pizza since it refers to pizza\n\n413\n00:17:22,980 --> 00:17:27,419\nand not oven\n\n414\n00:17:25,019 --> 00:17:29,639\nnote there's a lot to be said about\n\n415\n00:17:27,419 --> 00:17:31,500\ncalculating similarities in this context\n\n416\n00:17:29,640 --> 00:17:34,140\nand the dot product so if you're\n\n417\n00:17:31,500 --> 00:17:36,660\ninterested check out the quests anyway\n\n418\n00:17:34,140 --> 00:17:39,480\nsince let's is much more similar to\n\n419\n00:17:36,660 --> 00:17:41,580\nitself than it is to the word go\n\n420\n00:17:39,480 --> 00:17:43,798\nthen we want let's to have more\n\n421\n00:17:41,579 --> 00:17:45,058\ninfluence on its encoding than the word\n\n422\n00:17:43,798 --> 00:17:47,220\ngo\n\n423\n00:17:45,058 --> 00:17:48,960\nand we do this by first running the\n\n424\n00:17:47,220 --> 00:17:51,660\nsimilarities course through something\n\n425\n00:17:48,960 --> 00:17:54,779\ncalled a soft Max function\n\n426\n00:17:51,660 --> 00:17:56,820\nthe main idea of a soft Max function is\n\n427\n00:17:54,779 --> 00:17:59,700\nthat it preserves the order of the input\n\n428\n00:17:56,819 --> 00:18:02,399\nvalues from low to high and translates\n\n429\n00:17:59,700 --> 00:18:04,019\nthem into numbers between 0 and 1 that\n\n430\n00:18:02,400 --> 00:18:06,059\nadd up to one\n\n431\n00:18:04,019 --> 00:18:08,639\nso we can think of the output of the\n\n432\n00:18:06,058 --> 00:18:11,099\nsoftmax function as a way to determine\n\n433\n00:18:08,640 --> 00:18:13,860\nwhat percentage of each input word we\n\n434\n00:18:11,099 --> 00:18:16,500\nshould use to encode the word let's\n\n435\n00:18:13,859 --> 00:18:18,959\nin this case because let's is so much\n\n436\n00:18:16,500 --> 00:18:21,240\nmore similar to itself than the word go\n\n437\n00:18:18,960 --> 00:18:23,819\nwe'll use one hundred percent of the\n\n438\n00:18:21,240 --> 00:18:26,519\nword let's to encode less\n\n439\n00:18:23,819 --> 00:18:28,259\nand zero percent of the word go to\n\n440\n00:18:26,519 --> 00:18:30,599\nencode the word let's\n\n441\n00:18:28,259 --> 00:18:32,640\nnote there's a lot more to be said about\n\n442\n00:18:30,599 --> 00:18:34,798\nthe soft Max function so if you're\n\n443\n00:18:32,640 --> 00:18:37,980\ninterested check out the quest\n\n444\n00:18:34,798 --> 00:18:40,500\nanyway because we want 100 of the word\n\n445\n00:18:37,980 --> 00:18:43,140\nlet's to encode let's\n\n446\n00:18:40,500 --> 00:18:46,140\nwe create two more values that will\n\n447\n00:18:43,140 --> 00:18:47,400\ncleverly call values to represent the\n\n448\n00:18:46,140 --> 00:18:49,440\nword let's\n\n449\n00:18:47,400 --> 00:18:51,780\nand scale the values that represent\n\n450\n00:18:49,440 --> 00:18:54,120\nlet's by 1.0\n\n451\n00:18:51,779 --> 00:18:55,798\nthen we create two values to represent\n\n452\n00:18:54,119 --> 00:18:59,339\nthe word go\n\n453\n00:18:55,798 --> 00:19:02,819\nand scale those values by 0.0\n\n454\n00:18:59,339 --> 00:19:05,159\nlastly we add the scaled values together\n\n455\n00:19:02,819 --> 00:19:08,099\nand these sums which combine separate\n\n456\n00:19:05,160 --> 00:19:10,798\nencodings for both input words let's and\n\n457\n00:19:08,099 --> 00:19:13,859\ngo relative to their similarity to Let's\n\n458\n00:19:10,798 --> 00:19:15,960\nare the self-attention values for leads\n\n459\n00:19:13,859 --> 00:19:18,119\nbam\n\n460\n00:19:15,960 --> 00:19:20,700\nnow that we have self-attention values\n\n461\n00:19:18,119 --> 00:19:22,979\nfor the word let's it's time to\n\n462\n00:19:20,700 --> 00:19:25,319\ncalculate them for the word go\n\n463\n00:19:22,980 --> 00:19:28,919\nthe good news is that we don't need to\n\n464\n00:19:25,319 --> 00:19:31,139\nrecalculate the keys and values instead\n\n465\n00:19:28,919 --> 00:19:33,660\nall we need to do is create the query\n\n466\n00:19:31,140 --> 00:19:35,460\nthat represents the word go\n\n467\n00:19:33,660 --> 00:19:37,679\nand do the math\n\n468\n00:19:35,460 --> 00:19:40,079\nby first calculating the similarity\n\n469\n00:19:37,679 --> 00:19:42,900\nscores between the new query and the\n\n470\n00:19:40,079 --> 00:19:44,879\nkeys and then run the similarity scores\n\n471\n00:19:42,900 --> 00:19:47,220\nthrough a softmax\n\n472\n00:19:44,880 --> 00:19:49,380\nand then scale the values\n\n473\n00:19:47,220 --> 00:19:51,538\nand then add them together\n\n474\n00:19:49,380 --> 00:19:53,760\nand we end up with the self-attention\n\n475\n00:19:51,538 --> 00:19:56,700\nvalues for go\n\n476\n00:19:53,759 --> 00:19:59,519\nnote before we move on I want to point\n\n477\n00:19:56,700 --> 00:20:01,679\nout a few details about self-attention\n\n478\n00:19:59,519 --> 00:20:03,960\nfirst the weights that we use to\n\n479\n00:20:01,679 --> 00:20:06,780\ncalculate the self-attention queries are\n\n480\n00:20:03,960 --> 00:20:09,720\nthe exact same for let's and go\n\n481\n00:20:06,779 --> 00:20:11,038\nin other words this example uses one set\n\n482\n00:20:09,720 --> 00:20:13,679\nof weights for calculating\n\n483\n00:20:11,038 --> 00:20:16,019\nself-attention queries regardless of how\n\n484\n00:20:13,679 --> 00:20:18,840\nmany words are in the input\n\n485\n00:20:16,019 --> 00:20:21,418\nlikewise we reuse the sets of weights\n\n486\n00:20:18,839 --> 00:20:23,879\nfor calculating self-attention keys and\n\n487\n00:20:21,419 --> 00:20:26,038\nvalues for each input word\n\n488\n00:20:23,880 --> 00:20:28,679\nthis means that no matter how many words\n\n489\n00:20:26,038 --> 00:20:31,379\nare input into the Transformer\n\n490\n00:20:28,679 --> 00:20:34,080\nwe just reuse the same sets of weights\n\n491\n00:20:31,380 --> 00:20:35,460\nfor self-attention queries keys and\n\n492\n00:20:34,079 --> 00:20:37,500\nvalues\n\n493\n00:20:35,460 --> 00:20:39,660\nthe other thing I want to point out is\n\n494\n00:20:37,500 --> 00:20:42,179\nthat we can calculate the queries keys\n\n495\n00:20:39,660 --> 00:20:43,320\nand values for each word at the same\n\n496\n00:20:42,179 --> 00:20:45,419\ntime\n\n497\n00:20:43,319 --> 00:20:47,819\nin other words we don't have to\n\n498\n00:20:45,419 --> 00:20:50,520\ncalculate the query key and value for\n\n499\n00:20:47,819 --> 00:20:51,839\nthe first word first before moving on to\n\n500\n00:20:50,519 --> 00:20:53,819\nthe second word\n\n501\n00:20:51,839 --> 00:20:55,819\nand because we can do all of the\n\n502\n00:20:53,819 --> 00:20:58,019\ncomputation at the same time\n\n503\n00:20:55,819 --> 00:21:01,019\nTransformers can take advantage of\n\n504\n00:20:58,019 --> 00:21:03,119\nparallel Computing and run fast\n\n505\n00:21:01,019 --> 00:21:05,579\nnow that we understand the details of\n\n506\n00:21:03,119 --> 00:21:07,199\nhow self-attention Works let's shrink it\n\n507\n00:21:05,579 --> 00:21:11,399\ndown so we can keep building our\n\n508\n00:21:07,200 --> 00:21:13,919\nTransformer bam Josh you forgot\n\n509\n00:21:11,400 --> 00:21:16,320\nsomething if we want two values to\n\n510\n00:21:13,919 --> 00:21:18,419\nrepresent let's why don't we just use\n\n511\n00:21:16,319 --> 00:21:19,740\nthe two position encoded values we\n\n512\n00:21:18,419 --> 00:21:22,440\nstarted with\n\n513\n00:21:19,740 --> 00:21:25,019\nfirst the new self-attention values for\n\n514\n00:21:22,440 --> 00:21:27,240\neach word contain input from all of the\n\n515\n00:21:25,019 --> 00:21:30,298\nother words and this helps give each\n\n516\n00:21:27,240 --> 00:21:32,819\nword context and this can help establish\n\n517\n00:21:30,298 --> 00:21:33,960\nhow each word in the input is related to\n\n518\n00:21:32,819 --> 00:21:37,079\nthe others\n\n519\n00:21:33,960 --> 00:21:39,480\nalso if we think of this unit with its\n\n520\n00:21:37,079 --> 00:21:42,298\nweights for calculating queries keys and\n\n521\n00:21:39,480 --> 00:21:45,179\nvalues as a self-attention cell\n\n522\n00:21:42,298 --> 00:21:46,918\nthen in order to correctly establish how\n\n523\n00:21:45,179 --> 00:21:49,259\nwords are related in complicated\n\n524\n00:21:46,919 --> 00:21:51,840\nsentences and paragraphs\n\n525\n00:21:49,259 --> 00:21:54,839\nwe can create a stack of self-attention\n\n526\n00:21:51,839 --> 00:21:57,000\ncells each with its own sets of Weights\n\n527\n00:21:54,839 --> 00:21:59,220\nthat we apply to the position encoded\n\n528\n00:21:57,000 --> 00:22:02,279\nvalues for each word to capture\n\n529\n00:21:59,220 --> 00:22:04,500\ndifferent relationships among the words\n\n530\n00:22:02,279 --> 00:22:06,538\nin the manuscript that first describes\n\n531\n00:22:04,500 --> 00:22:08,460\nTransformers they stacked eight\n\n532\n00:22:06,538 --> 00:22:10,798\nself-attention cells and they called\n\n533\n00:22:08,460 --> 00:22:14,700\nthis multi-head attention\n\n534\n00:22:10,798 --> 00:22:15,798\nwhy eight instead of 12 or 16 I have no\n\n535\n00:22:14,700 --> 00:22:17,580\nidea\n\n536\n00:22:15,798 --> 00:22:20,279\nbam\n\n537\n00:22:17,579 --> 00:22:23,099\nokay going back to our simple example\n\n538\n00:22:20,279 --> 00:22:25,558\nwith only one self-attention cell\n\n539\n00:22:23,099 --> 00:22:27,178\nthere's one more thing we need to do to\n\n540\n00:22:25,558 --> 00:22:30,178\nencode the input\n\n541\n00:22:27,179 --> 00:22:32,280\nwe take the position encoded values\n\n542\n00:22:30,179 --> 00:22:35,220\nand add them to the self-attention\n\n543\n00:22:32,279 --> 00:22:37,440\nvalues these bypasses are called\n\n544\n00:22:35,220 --> 00:22:40,860\nresidual connections and they make it\n\n545\n00:22:37,440 --> 00:22:43,380\neasier to train complex neural networks\n\n546\n00:22:40,859 --> 00:22:45,479\nby allowing the self-attention layer to\n\n547\n00:22:43,380 --> 00:22:47,760\nestablish relationships among the input\n\n548\n00:22:45,480 --> 00:22:49,620\nwords without having to also preserve\n\n549\n00:22:47,759 --> 00:22:51,240\nthe word embedding and positioning\n\n550\n00:22:49,619 --> 00:22:52,619\ncoding information\n\n551\n00:22:51,240 --> 00:22:55,019\nbam\n\n552\n00:22:52,619 --> 00:22:58,019\nand that's all we need to do to encode\n\n553\n00:22:55,019 --> 00:23:02,519\nthe input for this simple Transformer\n\n554\n00:22:58,019 --> 00:23:04,859\ndouble bam note this simple Transformer\n\n555\n00:23:02,519 --> 00:23:07,700\nonly contains the parts required for\n\n556\n00:23:04,859 --> 00:23:11,819\nencoding the input word embedding\n\n557\n00:23:07,700 --> 00:23:13,740\npositional encoding self-attention and\n\n558\n00:23:11,819 --> 00:23:15,839\nresidual connections\n\n559\n00:23:13,740 --> 00:23:19,579\nthese four features allow the\n\n560\n00:23:15,839 --> 00:23:23,399\nTransformer to encode words into numbers\n\n561\n00:23:19,579 --> 00:23:26,399\nencode the positions of the words encode\n\n562\n00:23:23,400 --> 00:23:29,220\nthe relationships among the words and\n\n563\n00:23:26,400 --> 00:23:32,159\nrelatively easily and quickly train in\n\n564\n00:23:29,220 --> 00:23:34,558\nparallel that said there are lots of\n\n565\n00:23:32,159 --> 00:23:36,600\nextra things we can add to a Transformer\n\n566\n00:23:34,558 --> 00:23:40,980\nand we'll talk about those at the end of\n\n567\n00:23:36,599 --> 00:23:43,859\nthis Quest bam so now that we've encoded\n\n568\n00:23:40,980 --> 00:23:46,620\nthe English input phrase let's go it's\n\n569\n00:23:43,859 --> 00:23:49,019\ntime to decode it into Spanish\n\n570\n00:23:46,619 --> 00:23:52,019\nin other words the first part of a\n\n571\n00:23:49,019 --> 00:23:54,359\ntransformer is called an encoder and now\n\n572\n00:23:52,019 --> 00:23:55,619\nit's time to create the second part A\n\n573\n00:23:54,359 --> 00:23:58,918\ndecoder\n\n574\n00:23:55,619 --> 00:24:02,158\nthe decoder just like the encoder starts\n\n575\n00:23:58,919 --> 00:24:04,140\nwith word embedding however this time we\n\n576\n00:24:02,159 --> 00:24:06,659\ncreate embedding values for the output\n\n577\n00:24:04,140 --> 00:24:12,240\nvocabulary which consists of the Spanish\n\n578\n00:24:06,659 --> 00:24:14,039\nwords ear vamos e and the EOS end of\n\n579\n00:24:12,240 --> 00:24:16,798\nsequence token\n\n580\n00:24:14,038 --> 00:24:19,259\nnow because we just finished encoding\n\n581\n00:24:16,798 --> 00:24:21,658\nthe English sentence let's go\n\n582\n00:24:19,259 --> 00:24:25,140\nthe decoder starts with embedding values\n\n583\n00:24:21,659 --> 00:24:27,600\nfor the EOS token in this case we're\n\n584\n00:24:25,140 --> 00:24:30,120\nusing the EOS token to start the\n\n585\n00:24:27,599 --> 00:24:32,519\ndecoding because that is a common way to\n\n586\n00:24:30,119 --> 00:24:34,798\ninitialize the process of decoding the\n\n587\n00:24:32,519 --> 00:24:37,259\nencoded input sentence\n\n588\n00:24:34,798 --> 00:24:40,200\nhowever sometimes you'll see people use\n\n589\n00:24:37,259 --> 00:24:43,019\nSOS for startup sentence or start of\n\n590\n00:24:40,200 --> 00:24:46,440\nsequence to initialize the process\n\n591\n00:24:43,019 --> 00:24:48,658\nJosh starting with SOS makes more sense\n\n592\n00:24:46,440 --> 00:24:51,179\nto me then you can do it that way\n\n593\n00:24:48,659 --> 00:24:53,400\nSquatch I'm just saying a lot of people\n\n594\n00:24:51,179 --> 00:24:57,480\nstart with EOS\n\n595\n00:24:53,400 --> 00:24:58,798\nanyway we plug in 1 for Eos and zero for\n\n596\n00:24:57,480 --> 00:25:00,839\neverything else\n\n597\n00:24:58,798 --> 00:25:04,579\nand do the math\n\n598\n00:25:00,839 --> 00:25:07,798\nand we end up with 2.70 and negative\n\n599\n00:25:04,579 --> 00:25:10,558\n1.34 as the numbers that represent the\n\n600\n00:25:07,798 --> 00:25:12,599\nEOS token bam\n\n601\n00:25:10,558 --> 00:25:14,460\nnow let's shrink the word embedding down\n\n602\n00:25:12,599 --> 00:25:16,199\nto make more space\n\n603\n00:25:14,460 --> 00:25:17,519\nso that we can add the positional\n\n604\n00:25:16,200 --> 00:25:20,460\nencoding\n\n605\n00:25:17,519 --> 00:25:22,558\nnote these are the exact same sine and\n\n606\n00:25:20,460 --> 00:25:25,558\ncosine squiggles that we used when we\n\n607\n00:25:22,558 --> 00:25:28,139\nencoded the input and since the EOS\n\n608\n00:25:25,558 --> 00:25:30,298\ntoken is in the first position with two\n\n609\n00:25:28,140 --> 00:25:31,860\nembeddings we just add those two\n\n610\n00:25:30,298 --> 00:25:36,538\nposition values\n\n611\n00:25:31,859 --> 00:25:38,219\nand we get 2.70 and negative 0.34 as the\n\n612\n00:25:36,538 --> 00:25:42,720\nposition and word embedding values\n\n613\n00:25:38,220 --> 00:25:45,298\nrepresenting the EOS token bam now let's\n\n614\n00:25:42,720 --> 00:25:47,278\nconsolidate the math in the diagram\n\n615\n00:25:45,298 --> 00:25:50,038\nand before we move on to the next step\n\n616\n00:25:47,278 --> 00:25:51,720\nlet's review a key concept from when we\n\n617\n00:25:50,038 --> 00:25:54,480\nencoded the input\n\n618\n00:25:51,720 --> 00:25:56,759\none key concept from earlier was that we\n\n619\n00:25:54,480 --> 00:25:58,140\ncreated a single unit to process an\n\n620\n00:25:56,759 --> 00:26:00,359\ninput word\n\n621\n00:25:58,140 --> 00:26:02,340\nand then we just copied that unit for\n\n622\n00:26:00,359 --> 00:26:04,859\neach word in the input\n\n623\n00:26:02,339 --> 00:26:07,439\nand if we had more words we just make\n\n624\n00:26:04,859 --> 00:26:09,839\nmore copies of the same unit\n\n625\n00:26:07,440 --> 00:26:12,538\nby creating a single unit that can be\n\n626\n00:26:09,839 --> 00:26:14,099\ncopied for each input word the\n\n627\n00:26:12,538 --> 00:26:16,259\nTransformer can do all of the\n\n628\n00:26:14,099 --> 00:26:18,359\ncomputation for each word in the input\n\n629\n00:26:16,259 --> 00:26:20,700\nat the same time\n\n630\n00:26:18,359 --> 00:26:22,979\nfor example we can calculate the word\n\n631\n00:26:20,700 --> 00:26:24,298\nembeddings on different processors at\n\n632\n00:26:22,980 --> 00:26:26,759\nthe same time\n\n633\n00:26:24,298 --> 00:26:28,259\nand then add the positional encoding at\n\n634\n00:26:26,759 --> 00:26:30,900\nthe same time\n\n635\n00:26:28,259 --> 00:26:33,119\nand then calculate the queries keys and\n\n636\n00:26:30,900 --> 00:26:35,940\nvalues at the same time\n\n637\n00:26:33,119 --> 00:26:38,038\nand once that is done we can calculate\n\n638\n00:26:35,940 --> 00:26:39,120\nthe self-attention values at the same\n\n639\n00:26:38,038 --> 00:26:42,359\ntime\n\n640\n00:26:39,119 --> 00:26:44,819\nand lastly we can calculate the residual\n\n641\n00:26:42,359 --> 00:26:47,158\nconnections at the same time\n\n642\n00:26:44,819 --> 00:26:49,019\ndoing all of the computations at the\n\n643\n00:26:47,159 --> 00:26:51,360\nsame time rather than doing them\n\n644\n00:26:49,019 --> 00:26:53,579\nsequentially for each word\n\n645\n00:26:51,359 --> 00:26:56,099\nmeans we can process a lot of words\n\n646\n00:26:53,579 --> 00:26:59,158\nrelatively quickly on a chip with a lot\n\n647\n00:26:56,099 --> 00:27:02,099\nof computing cores like a GPU Graphics\n\n648\n00:26:59,159 --> 00:27:03,240\nProcessing Unit or multiple chips in the\n\n649\n00:27:02,099 --> 00:27:06,178\ncloud\n\n650\n00:27:03,240 --> 00:27:08,460\nwell likewise when we decode and\n\n651\n00:27:06,179 --> 00:27:10,200\ntranslate the input we want a single\n\n652\n00:27:08,460 --> 00:27:12,960\nunit that we can copy for each\n\n653\n00:27:10,200 --> 00:27:15,120\ntranslated word for the same reasons we\n\n654\n00:27:12,960 --> 00:27:18,120\nwant to do the math quickly\n\n655\n00:27:15,119 --> 00:27:21,599\nso even though we're only processing the\n\n656\n00:27:18,119 --> 00:27:24,778\nEOS token so far we add a self-attention\n\n657\n00:27:21,599 --> 00:27:27,538\nlayer so that ultimately we can keep\n\n658\n00:27:24,778 --> 00:27:30,058\ntrack of related words in the output\n\n659\n00:27:27,538 --> 00:27:33,240\nnow that we have the query key and value\n\n660\n00:27:30,058 --> 00:27:36,418\nnumbers for the EOS token we calculate\n\n661\n00:27:33,240 --> 00:27:38,579\nitself attention values just like before\n\n662\n00:27:36,419 --> 00:27:41,880\nand the self-attention values for the\n\n663\n00:27:38,579 --> 00:27:46,079\nEOS token are negative 2.8 and negative\n\n664\n00:27:41,880 --> 00:27:48,000\n2.3 note the sets of Weights we used to\n\n665\n00:27:46,079 --> 00:27:50,879\ncalculate the decoder self-attention\n\n666\n00:27:48,000 --> 00:27:53,339\nquery key and value are different from\n\n667\n00:27:50,880 --> 00:27:56,580\nthe sets we used in the encoder\n\n668\n00:27:53,339 --> 00:27:59,519\nnow let's consolidate the math and add\n\n669\n00:27:56,579 --> 00:28:01,139\nresidual connections just like before\n\n670\n00:27:59,519 --> 00:28:03,599\nbam\n\n671\n00:28:01,140 --> 00:28:05,640\nnow so far we've talked about how\n\n672\n00:28:03,599 --> 00:28:07,459\nself-attention helps the Transformer\n\n673\n00:28:05,640 --> 00:28:09,900\nkeep track of how words are related\n\n674\n00:28:07,460 --> 00:28:12,179\nwithin a sentence\n\n675\n00:28:09,900 --> 00:28:14,519\nhowever since We're translating a\n\n676\n00:28:12,179 --> 00:28:16,860\nsentence we also need to keep track of\n\n677\n00:28:14,519 --> 00:28:18,900\nthe relationships between the input\n\n678\n00:28:16,859 --> 00:28:22,439\nsentence and the output\n\n679\n00:28:18,900 --> 00:28:25,259\nfor example if the input sentence was\n\n680\n00:28:22,440 --> 00:28:28,620\ndon't eat the delicious looking and\n\n681\n00:28:25,259 --> 00:28:31,140\nsmelling pizza then when translating\n\n682\n00:28:28,619 --> 00:28:33,658\nit's super important to keep track of\n\n683\n00:28:31,140 --> 00:28:36,120\nthe very first word don't\n\n684\n00:28:33,659 --> 00:28:38,159\nif the translation focuses on other\n\n685\n00:28:36,119 --> 00:28:40,439\nparts of the sentence and omits the\n\n686\n00:28:38,159 --> 00:28:43,140\ndon't then we'll end up with\n\n687\n00:28:40,440 --> 00:28:44,159\neat the delicious looking and smelling\n\n688\n00:28:43,140 --> 00:28:46,799\nPizza\n\n689\n00:28:44,159 --> 00:28:48,600\nand these two sentences have completely\n\n690\n00:28:46,798 --> 00:28:51,119\nopposite meanings\n\n691\n00:28:48,599 --> 00:28:53,038\nso it's super important for the decoder\n\n692\n00:28:51,119 --> 00:28:54,719\nto keep track of the significant words\n\n693\n00:28:53,038 --> 00:28:57,720\nin the input\n\n694\n00:28:54,720 --> 00:29:00,298\nso the main idea of encoder decoder\n\n695\n00:28:57,720 --> 00:29:02,640\nattention is to allow the decoder to\n\n696\n00:29:00,298 --> 00:29:04,259\nkeep track of the significant words in\n\n697\n00:29:02,640 --> 00:29:06,720\nthe input\n\n698\n00:29:04,259 --> 00:29:09,658\nnow that we know the main idea behind\n\n699\n00:29:06,720 --> 00:29:10,860\nencoder decoder attention here are the\n\n700\n00:29:09,659 --> 00:29:13,020\ndetails\n\n701\n00:29:10,859 --> 00:29:15,058\nfirst to give us a little more room\n\n702\n00:29:13,019 --> 00:29:16,200\nlet's consolidate the math and the\n\n703\n00:29:15,058 --> 00:29:18,839\ndiagrams\n\n704\n00:29:16,200 --> 00:29:21,419\nnow just like we did for self-attention\n\n705\n00:29:18,839 --> 00:29:24,298\nwe create two new values to represent\n\n706\n00:29:21,419 --> 00:29:27,480\nthe query for the EOS token in the\n\n707\n00:29:24,298 --> 00:29:30,599\ndecoder then we create keys for each\n\n708\n00:29:27,480 --> 00:29:33,360\nword in the encoder and we calculate the\n\n709\n00:29:30,599 --> 00:29:36,778\nsimilarities between the EOS token in\n\n710\n00:29:33,359 --> 00:29:39,178\nthe decoder and each word in the encoder\n\n711\n00:29:36,778 --> 00:29:40,440\nby calculating the dot products just\n\n712\n00:29:39,179 --> 00:29:42,720\nlike before\n\n713\n00:29:40,440 --> 00:29:45,659\nthen we run the similarities through a\n\n714\n00:29:42,720 --> 00:29:47,579\nsoftmax function and this tells us to\n\n715\n00:29:45,659 --> 00:29:50,220\nuse one hundred percent of the first\n\n716\n00:29:47,579 --> 00:29:53,278\ninput word and zero percent of the\n\n717\n00:29:50,220 --> 00:29:56,038\nsecond when the decoder determines what\n\n718\n00:29:53,278 --> 00:29:58,619\nshould be the first translated word\n\n719\n00:29:56,038 --> 00:30:01,019\nnow that we know what percentage of each\n\n720\n00:29:58,619 --> 00:30:03,658\ninput word to use when determining what\n\n721\n00:30:01,019 --> 00:30:07,319\nshould be the first translated word\n\n722\n00:30:03,659 --> 00:30:09,778\nwe calculate values for each input word\n\n723\n00:30:07,319 --> 00:30:11,759\nand then scale those values by the soft\n\n724\n00:30:09,778 --> 00:30:14,099\nMax percentages\n\n725\n00:30:11,759 --> 00:30:16,500\nand then add the pairs of scaled values\n\n726\n00:30:14,099 --> 00:30:18,178\ntogether to get the encoder decoder\n\n727\n00:30:16,500 --> 00:30:19,558\nattention values\n\n728\n00:30:18,179 --> 00:30:22,259\nbam\n\n729\n00:30:19,558 --> 00:30:24,240\nnow to make room for the next step let's\n\n730\n00:30:22,259 --> 00:30:26,339\nconsolidate the encoder decoder\n\n731\n00:30:24,240 --> 00:30:28,919\nattention in our diagram\n\n732\n00:30:26,339 --> 00:30:31,139\nnote the sets of Weights that we use to\n\n733\n00:30:28,919 --> 00:30:33,778\ncalculate the queries keys and values\n\n734\n00:30:31,140 --> 00:30:35,520\nfor encoder decoder attention are\n\n735\n00:30:33,778 --> 00:30:37,380\ndifferent from the sets of Weights we\n\n736\n00:30:35,519 --> 00:30:40,319\nuse for self-attention\n\n737\n00:30:37,380 --> 00:30:42,600\nhowever just like for self-attention the\n\n738\n00:30:40,319 --> 00:30:44,158\nsets of Weights are copied and reused\n\n739\n00:30:42,599 --> 00:30:46,259\nfor each word\n\n740\n00:30:44,159 --> 00:30:48,480\nthis allows the Transformer to be\n\n741\n00:30:46,259 --> 00:30:50,220\nflexible with the length of the inputs\n\n742\n00:30:48,480 --> 00:30:53,339\nand outputs\n\n743\n00:30:50,220 --> 00:30:55,140\nand also we can stack encode or decoder\n\n744\n00:30:53,339 --> 00:30:58,079\nattention just like we can stack\n\n745\n00:30:55,140 --> 00:31:01,080\nself-attention to keep track of words in\n\n746\n00:30:58,079 --> 00:31:03,480\ncomplicated phrases bam\n\n747\n00:31:01,079 --> 00:31:04,678\nnow we add another set of residual\n\n748\n00:31:03,480 --> 00:31:07,319\nconnections\n\n749\n00:31:04,679 --> 00:31:09,480\nthat allow the encoder decoder attention\n\n750\n00:31:07,319 --> 00:31:11,519\nto focus on the relationships between\n\n751\n00:31:09,480 --> 00:31:13,440\nthe output words and the input words\n\n752\n00:31:11,519 --> 00:31:15,720\nwithout having to preserve the\n\n753\n00:31:13,440 --> 00:31:19,140\nself-attention or word and position\n\n754\n00:31:15,720 --> 00:31:21,839\nencoding that happened earlier then we\n\n755\n00:31:19,140 --> 00:31:24,720\nconsolidate the math and the diagram\n\n756\n00:31:21,839 --> 00:31:27,359\nlastly we need a way to take these two\n\n757\n00:31:24,720 --> 00:31:30,419\nvalues that represent the EOS token in\n\n758\n00:31:27,359 --> 00:31:35,339\nthe decoder and select one of the four\n\n759\n00:31:30,419 --> 00:31:37,620\noutput tokens ear vamos e or EOS\n\n760\n00:31:35,339 --> 00:31:39,839\nso we run these two values through a\n\n761\n00:31:37,619 --> 00:31:41,819\nfully connected layer that has one input\n\n762\n00:31:39,839 --> 00:31:44,398\nfor each value that represents the\n\n763\n00:31:41,819 --> 00:31:45,898\ncurrent token so in this case we have\n\n764\n00:31:44,398 --> 00:31:48,418\ntwo inputs\n\n765\n00:31:45,898 --> 00:31:50,639\nand one output for each token in the\n\n766\n00:31:48,419 --> 00:31:52,860\noutput vocabulary which in this case\n\n767\n00:31:50,640 --> 00:31:55,679\nmeans four outputs\n\n768\n00:31:52,859 --> 00:31:57,418\nnote a fully connected layer is just a\n\n769\n00:31:55,679 --> 00:32:00,298\nsimple neural network with weights\n\n770\n00:31:57,419 --> 00:32:03,600\nnumbers we multiply the inputs by\n\n771\n00:32:00,298 --> 00:32:04,918\nand biases numbers we add to the sums of\n\n772\n00:32:03,599 --> 00:32:07,139\nthe products\n\n773\n00:32:04,919 --> 00:32:08,880\nand when we do the math we get four\n\n774\n00:32:07,140 --> 00:32:11,038\noutput values\n\n775\n00:32:08,880 --> 00:32:13,500\nwhich we run through a final soft Max\n\n776\n00:32:11,038 --> 00:32:15,679\nfunction to select the first output word\n\n777\n00:32:13,500 --> 00:32:17,519\nvamos\n\n778\n00:32:15,679 --> 00:32:20,460\nbam\n\n779\n00:32:17,519 --> 00:32:25,980\nnote vamos is the Spanish translation\n\n780\n00:32:20,460 --> 00:32:28,919\nfor Let's Go triple boom no not yet\n\n781\n00:32:25,980 --> 00:32:30,899\nso far the translation is correct but\n\n782\n00:32:28,919 --> 00:32:34,080\nthe decoder doesn't stop until it\n\n783\n00:32:30,898 --> 00:32:36,298\noutputs an EOS token so let's\n\n784\n00:32:34,079 --> 00:32:39,480\nconsolidate our diagrams\n\n785\n00:32:36,298 --> 00:32:41,879\nand plug the translated word vamos into\n\n786\n00:32:39,480 --> 00:32:43,620\na copy of the decoder's embedding layer\n\n787\n00:32:41,880 --> 00:32:45,840\nand do the math\n\n788\n00:32:43,619 --> 00:32:47,038\nfirst we get the word embeddings for\n\n789\n00:32:45,839 --> 00:32:50,038\nvamos\n\n790\n00:32:47,038 --> 00:32:52,440\nthen we add the positional encoding\n\n791\n00:32:50,038 --> 00:32:54,898\nnow we calculate self-attention values\n\n792\n00:32:52,440 --> 00:32:57,659\nfor vamos using the exact same weights\n\n793\n00:32:54,898 --> 00:33:00,418\nthat we used for the EOS token\n\n794\n00:32:57,659 --> 00:33:02,760\nnow add the residual connections\n\n795\n00:33:00,419 --> 00:33:05,038\nand calculate the encoder decoder\n\n796\n00:33:02,759 --> 00:33:07,798\nattention using the same sets of Weights\n\n797\n00:33:05,038 --> 00:33:10,679\nthat we used for the EOS token\n\n798\n00:33:07,798 --> 00:33:13,139\nnow we add more residual connections\n\n799\n00:33:10,679 --> 00:33:15,298\nlastly we run the values that represent\n\n800\n00:33:13,140 --> 00:33:17,940\nvamos through the same fully connected\n\n801\n00:33:15,298 --> 00:33:21,240\nlayer and softmax that we used for the\n\n802\n00:33:17,940 --> 00:33:25,259\nEOS token and the second output from the\n\n803\n00:33:21,240 --> 00:33:27,960\ndecoder is eos so we are done decoding\n\n804\n00:33:25,259 --> 00:33:30,480\ntriple bam\n\n805\n00:33:27,960 --> 00:33:32,700\nat long last we've shown how a\n\n806\n00:33:30,480 --> 00:33:34,798\nTransformer can encode a simple input\n\n807\n00:33:32,700 --> 00:33:37,500\nphrase let's go\n\n808\n00:33:34,798 --> 00:33:40,139\nand decode the encoding into the\n\n809\n00:33:37,500 --> 00:33:41,778\ntranslated phrase of vamos\n\n810\n00:33:40,140 --> 00:33:44,159\nin summary\n\n811\n00:33:41,778 --> 00:33:46,259\nTransformers use word embedding to\n\n812\n00:33:44,159 --> 00:33:48,720\nconvert words into numbers\n\n813\n00:33:46,259 --> 00:33:50,058\npositional encoding to keep track of\n\n814\n00:33:48,720 --> 00:33:52,380\nword order\n\n815\n00:33:50,058 --> 00:33:54,720\nself-attention to keep track of word\n\n816\n00:33:52,380 --> 00:33:56,278\nrelationships within the input and\n\n817\n00:33:54,720 --> 00:33:59,038\noutput phrases\n\n818\n00:33:56,278 --> 00:34:01,259\nencoder decoder attention to keep track\n\n819\n00:33:59,038 --> 00:34:03,480\nof things between the input and output\n\n820\n00:34:01,259 --> 00:34:05,940\nphrases to make sure that important\n\n821\n00:34:03,480 --> 00:34:07,200\nwords in the input are not lost in the\n\n822\n00:34:05,940 --> 00:34:09,599\ntranslation\n\n823\n00:34:07,200 --> 00:34:12,599\nand residual connections to allow each\n\n824\n00:34:09,599 --> 00:34:15,300\nsubunit like self-attention to focus on\n\n825\n00:34:12,599 --> 00:34:18,000\nsolving just one part of the problem\n\n826\n00:34:15,300 --> 00:34:20,460\nnow that we understand the main ideas of\n\n827\n00:34:18,000 --> 00:34:22,858\nhow Transformers work let's talk about a\n\n828\n00:34:20,460 --> 00:34:25,320\nfew extra things we can add to them\n\n829\n00:34:22,858 --> 00:34:26,398\nin this example we kept things super\n\n830\n00:34:25,320 --> 00:34:29,039\nsimple\n\n831\n00:34:26,398 --> 00:34:31,980\nhowever if we had larger vocabularies\n\n832\n00:34:29,039 --> 00:34:34,320\nand the original Transformer had 37 000\n\n833\n00:34:31,980 --> 00:34:35,519\ntokens and longer input and output\n\n834\n00:34:34,320 --> 00:34:37,980\nphrases\n\n835\n00:34:35,519 --> 00:34:40,259\nthen in order to get their model to work\n\n836\n00:34:37,980 --> 00:34:41,760\nthey had to normalize the values after\n\n837\n00:34:40,260 --> 00:34:44,280\nevery step\n\n838\n00:34:41,760 --> 00:34:46,139\nfor example they normalize the values\n\n839\n00:34:44,280 --> 00:34:48,659\nafter positional encoding and after\n\n840\n00:34:46,139 --> 00:34:51,780\nself-attention in both the encoder and\n\n841\n00:34:48,659 --> 00:34:54,480\nthe decoder also when we calculated\n\n842\n00:34:51,780 --> 00:34:57,060\nattention values we used the dot product\n\n843\n00:34:54,480 --> 00:34:59,280\nto calculate the similarities but you\n\n844\n00:34:57,059 --> 00:35:00,299\ncan use whatever similarity function you\n\n845\n00:34:59,280 --> 00:35:02,760\nwant\n\n846\n00:35:00,300 --> 00:35:05,099\nin the original Transformer manuscript\n\n847\n00:35:02,760 --> 00:35:07,320\nthey calculated the similarities with a\n\n848\n00:35:05,099 --> 00:35:09,300\nDOT product divided by the square root\n\n849\n00:35:07,320 --> 00:35:12,180\nof the number of embedding values per\n\n850\n00:35:09,300 --> 00:35:14,400\ntoken just like with scaling the values\n\n851\n00:35:12,179 --> 00:35:16,858\nafter each step they found that scaling\n\n852\n00:35:14,400 --> 00:35:19,740\nthe dot product helped encode and decode\n\n853\n00:35:16,858 --> 00:35:22,259\nlong and complicated phrases\n\n854\n00:35:19,739 --> 00:35:24,539\nlastly to give a Transformer more\n\n855\n00:35:22,260 --> 00:35:26,940\nweights and biases to fit to complicated\n\n856\n00:35:24,539 --> 00:35:28,800\ndata you can add additional neural\n\n857\n00:35:26,940 --> 00:35:31,858\nnetworks with hidden layers to both the\n\n858\n00:35:28,800 --> 00:35:34,039\nencoder and decoder bam\n\n\"\"\".strip()\n\nresponse = generate_response(prompt, max_new_tokens=1024)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:56:02.082662Z","iopub.execute_input":"2023-10-08T20:56:02.082990Z","iopub.status.idle":"2023-10-08T20:56:04.413617Z","shell.execute_reply.started":"2023-10-08T20:56:02.082964Z","shell.execute_reply":"2023-10-08T20:56:04.412687Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","File \u001b[0;32m<timed exec>:3414\u001b[0m\n","Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m answer_tokens \u001b[38;5;241m=\u001b[39m outputs[:, encoding\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] :]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(answer_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2642\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2642\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:652\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    650\u001b[0m         (batch_size, seq_length_with_past), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    651\u001b[0m     )\n\u001b[0;32m--> 652\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_decoder_attention_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:580\u001b[0m, in \u001b[0;36mLlamaModel._prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    578\u001b[0m combined_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 580\u001b[0m     combined_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_make_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     expanded_attn_mask \u001b[38;5;241m=\u001b[39m _expand_mask(attention_mask, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    590\u001b[0m         inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    591\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:53\u001b[0m, in \u001b[0;36m_make_causal_mask\u001b[0;34m(input_ids_shape, dtype, device, past_key_values_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m mask_cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     52\u001b[0m mask\u001b[38;5;241m.\u001b[39mmasked_fill_(mask_cond \u001b[38;5;241m<\u001b[39m (mask_cond \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     56\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros(tgt_len, past_key_values_length, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice), mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 15.90 GiB total capacity; 13.36 GiB already allocated; 1.33 GiB free; 13.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 15.90 GiB total capacity; 13.36 GiB already allocated; 1.33 GiB free; 13.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2023-10-08T20:54:48.692346Z","iopub.execute_input":"2023-10-08T20:54:48.693650Z","iopub.status.idle":"2023-10-08T20:54:48.722720Z","shell.execute_reply.started":"2023-10-08T20:54:48.693617Z","shell.execute_reply":"2023-10-08T20:54:48.721719Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"],"ename":"NameError","evalue":"name 'response' is not defined","output_type":"error"}]}]}