{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-16T23:32:10.329707Z","iopub.execute_input":"2024-04-16T23:32:10.330187Z","iopub.status.idle":"2024-04-16T23:32:10.336990Z","shell.execute_reply.started":"2024-04-16T23:32:10.330145Z","shell.execute_reply":"2024-04-16T23:32:10.335967Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/jwkirchenbauer/lm-watermarking","metadata":{"execution":{"iopub.status.idle":"2024-04-16T23:32:11.319148Z","shell.execute_reply.started":"2024-04-16T23:32:10.339517Z","shell.execute_reply":"2024-04-16T23:32:11.317933Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"fatal: destination path 'lm-watermarking' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/lm-watermarking\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:11.320814Z","iopub.execute_input":"2024-04-16T23:32:11.321194Z","iopub.status.idle":"2024-04-16T23:32:11.326422Z","shell.execute_reply.started":"2024-04-16T23:32:11.321157Z","shell.execute_reply":"2024-04-16T23:32:11.325467Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"!pip install torch transformers accelerate optimum","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:11.328496Z","iopub.execute_input":"2024-04-16T23:32:11.328819Z","iopub.status.idle":"2024-04-16T23:32:24.097341Z","shell.execute_reply.started":"2024-04-16T23:32:11.328795Z","shell.execute_reply":"2024-04-16T23:32:24.096132Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.18.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom optimum.bettertransformer import BetterTransformer","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:24.098984Z","iopub.execute_input":"2024-04-16T23:32:24.099281Z","iopub.status.idle":"2024-04-16T23:32:24.104618Z","shell.execute_reply.started":"2024-04-16T23:32:24.099255Z","shell.execute_reply":"2024-04-16T23:32:24.103628Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from extended_watermark_processor import WatermarkLogitsProcessor\nfrom transformers import LogitsProcessor,LogitsProcessorList\n\n# model_name = \"facebook/opt-350m\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n# watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n#                                                gamma=0.25,\n#                                                delta=2.0,\n#                                                seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n# # Note:\n# # You can turn off self-hashing by setting the seeding scheme to `minhash`.\n# input_text = \"A good thesis\"\n# tokenized_input = tokenizer(input_text, return_tensors='pt').to(model.device)\n# # note that if the model is on cuda, then the input is on cuda\n# # and thus the watermarking rng is cuda-based.\n# # This is a different generator than the cpu-based rng in pytorch!\n\n# output_tokens = model.generate(**tokenized_input,\n#                                logits_processor=LogitsProcessorList([watermark_processor]))\n\n# # if decoder only model, then we need to isolate the\n# # newly generated tokens as only those are watermarked, the input/prompt is not\n# output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n\n# output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:24.105785Z","iopub.execute_input":"2024-04-16T23:32:24.106061Z","iopub.status.idle":"2024-04-16T23:32:24.115060Z","shell.execute_reply.started":"2024-04-16T23:32:24.106038Z","shell.execute_reply":"2024-04-16T23:32:24.114101Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# print(output_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:24.116111Z","iopub.execute_input":"2024-04-16T23:32:24.116377Z","iopub.status.idle":"2024-04-16T23:32:24.128500Z","shell.execute_reply.started":"2024-04-16T23:32:24.116355Z","shell.execute_reply":"2024-04-16T23:32:24.127583Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\nfrom extended_watermark_processor import WatermarkDetector\n\n# watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n#                                         gamma=0.25, # should match original setting\n#                                         seeding_scheme=\"selfhash\", # should match original setting\n#                                         device=model.device, # must match the original rng device type\n#                                         tokenizer=tokenizer,\n#                                         z_threshold=4.0,\n#                                         normalizers=[],\n#                                         ignore_repeated_ngrams=True)\n\n# score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:48.380138Z","iopub.execute_input":"2024-04-16T23:32:48.380496Z","iopub.status.idle":"2024-04-16T23:32:48.385395Z","shell.execute_reply.started":"2024-04-16T23:32:48.380469Z","shell.execute_reply":"2024-04-16T23:32:48.384360Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# print(score_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed,AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\n# from tqdm import tqdm\nfrom optimum.bettertransformer import BetterTransformer\nfrom tqdm.auto import tqdm  \nimport os, argparse","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:32:53.942947Z","iopub.execute_input":"2024-04-16T23:32:53.943324Z","iopub.status.idle":"2024-04-16T23:32:53.948621Z","shell.execute_reply.started":"2024-04-16T23:32:53.943296Z","shell.execute_reply":"2024-04-16T23:32:53.947707Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!pip install undecorated","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:33:23.817828Z","iopub.execute_input":"2024-04-16T23:33:23.818181Z","iopub.status.idle":"2024-04-16T23:33:36.529558Z","shell.execute_reply.started":"2024-04-16T23:33:23.818155Z","shell.execute_reply":"2024-04-16T23:33:36.528394Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Collecting undecorated\n  Downloading undecorated-0.3.0-py3-none-any.whl.metadata (2.5 kB)\nDownloading undecorated-0.3.0-py3-none-any.whl (4.8 kB)\nInstalling collected packages: undecorated\nSuccessfully installed undecorated-0.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from undecorated import undecorated\nfrom types import MethodType\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:34:18.163507Z","iopub.execute_input":"2024-04-16T23:34:18.164430Z","iopub.status.idle":"2024-04-16T23:34:18.168645Z","shell.execute_reply.started":"2024-04-16T23:34:18.164398Z","shell.execute_reply":"2024-04-16T23:34:18.167745Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# # Load your model and tokenizer\n# # model_name = \"facebook/opt-350m\"\n# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n# # model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n\n# model_name = \"facebook/opt-350m\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n# watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n#                                                gamma=0.25,\n#                                                delta=2.0,\n#                                                seeding_scheme=\"selfhash\")\n\n# correct = 0  # Number of correct predictions\n# total = 0  # Total number of predictions\n\n# # Open the HellaSwag jsonl file\n# with open('/kaggle/working/hellaswag/data/hellaswag_val.jsonl', 'r') as f:\n#   # Loop through each line in the file\n#   for line in tqdm(f, desc=\"Evaluating\"):\n#     item = json.loads(line)  # Parse each line as a JSON object\n\n#     context = item['ctx']  # Use 'ctx' field as context\n#     endings = item['endings']  # Possible endings\n#     correct_end = item['label']  # Correct answer index is in 'label' field\n\n#     # Generate a score for each possible ending\n#     scores = []\n#     for ending in endings:\n#         input_text = context + ' ' + ending  # Combine context and ending\n#         input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\"cuda\")  # Tokenize input\n        \n#         with torch.no_grad():\n#             outputs = model(input_ids, labels=input_ids)  # Get model output\n#             loss = outputs.loss  # Get loss value\n#             scores.append(loss.item())  # Add loss value to scores list\n\n#     # Choose the most likely ending based on the scores (lowest loss)\n#     predicted_end = scores.index(min(scores))\n\n#     # Update the number of correct predictions\n#     correct += 1 if predicted_end == correct_end else 0\n#     total += 1  # Update the total number of predictions\n\n# # Print the final accuracy after the loop\n# print(f\"Final Accuracy: {correct / total:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:34:26.004618Z","iopub.execute_input":"2024-04-16T23:34:26.005000Z","iopub.status.idle":"2024-04-16T23:34:26.011439Z","shell.execute_reply.started":"2024-04-16T23:34:26.004971Z","shell.execute_reply":"2024-04-16T23:34:26.010607Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/rowanz/hellaswag","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:34:28.366302Z","iopub.execute_input":"2024-04-16T23:34:28.367139Z","iopub.status.idle":"2024-04-16T23:34:32.410029Z","shell.execute_reply.started":"2024-04-16T23:34:28.367107Z","shell.execute_reply":"2024-04-16T23:34:32.408768Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Cloning into 'hellaswag'...\nremote: Enumerating objects: 45, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 45 (delta 0), reused 2 (delta 0), pack-reused 41\u001b[K\nUnpacking objects: 100% (45/45), 17.69 MiB | 8.21 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load your model and tokenizer\nmodel_name = \"facebook/opt-350m\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n# Define watermark processor\nwatermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n                                               gamma=0.25,\n                                               delta=2.0,\n                                               seeding_scheme=\"selfhash\")\n\ncorrect = 0  # Number of correct predictions\ntotal = 0  # Total number of predictions\n\n# Open the HellaSwag jsonl file\nwith open('/kaggle/working/hellaswag/data/hellaswag_val.jsonl', 'r') as f:\n  # Loop through each line in the file\n  for line in tqdm(f, desc=\"Evaluating\"):\n    item = json.loads(line)  # Parse each line as a JSON object\n\n    context = item['ctx']  # Use 'ctx' field as context\n    endings = item['endings']  # Possible endings\n    correct_end = item['label']  # Correct answer index is in 'label' field\n\n    # Generate scores for each possible ending with watermarking\n    scores = []\n    for ending in endings:\n        input_text = context + ' ' + ending  # Combine context and ending\n\n        # Tokenize input\n        tokenized_input = tokenizer(input_text, return_tensors='pt').to(model.device)\n\n        # Generate with watermark embedding\n        with torch.no_grad():\n            output_tokens = model.generate(**tokenized_input,\n                                        logits_processor=LogitsProcessorList([watermark_processor]),max_length =100, return_dict_in_generate=True, output_scores=True)\n            print(type(output_tokens))\n        # Isolate newly generated tokens (decoder only model)\n            if isinstance(model, AutoModelForSeq2SeqLM):\n                output_tokens = output_tokens[:, tokenized_input[\"input_ids\"].shape[-1]:]\n\n        # Get loss (assuming lower loss = better fit)\n#         print(output_tokens.scores)\n        loss = output_tokens.sequences\n        scores.append(loss)\n\n    # Choose most likely ending based on the scores (lowest loss)\n    predicted_end = scores.index(min(scores))\n\n    # Update accuracy metrics\n    correct += 1 if predicted_end == correct_end else 0\n    total += 1\n    print(\"Accuracy({}/{}): {}\".format(correct, total, correct / total), end='\\r')\n# Print the final accuracy after the loop\nprint(f\"Final Accuracy: {correct / total:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-17T00:27:34.844227Z","iopub.execute_input":"2024-04-17T00:27:34.845162Z","iopub.status.idle":"2024-04-17T00:28:36.770946Z","shell.execute_reply.started":"2024-04-17T00:27:34.845127Z","shell.execute_reply":"2024-04-17T00:28:36.769464Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d607d2d5c5742419d09651c79845216"}},"metadata":{}},{"name":"stdout","text":"<class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n<class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n<class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n<class 'transformers.generation.utils.GenerateDecoderOnlyOutput'>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Choose most likely ending based on the scores (lowest loss)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m predicted_end \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Update accuracy metrics\u001b[39;00m\n\u001b[1;32m     50\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predicted_end \u001b[38;5;241m==\u001b[39m correct_end \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n","\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"],"ename":"RuntimeError","evalue":"Boolean value of Tensor with more than one value is ambiguous","output_type":"error"}]},{"cell_type":"code","source":"# print(output_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:51:22.683836Z","iopub.execute_input":"2024-04-16T23:51:22.684589Z","iopub.status.idle":"2024-04-16T23:51:22.688397Z","shell.execute_reply.started":"2024-04-16T23:51:22.684558Z","shell.execute_reply":"2024-04-16T23:51:22.687446Z"},"trusted":true},"execution_count":60,"outputs":[]}]}