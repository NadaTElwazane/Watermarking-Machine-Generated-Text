{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T22:00:47.046464Z","iopub.execute_input":"2024-03-19T22:00:47.046928Z","iopub.status.idle":"2024-03-19T22:00:47.474248Z","shell.execute_reply.started":"2024-03-19T22:00:47.046896Z","shell.execute_reply":"2024-03-19T22:00:47.472867Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/jthickstun/watermark.git","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:00:49.698149Z","iopub.execute_input":"2024-03-19T22:00:49.699182Z","iopub.status.idle":"2024-03-19T22:00:51.791771Z","shell.execute_reply.started":"2024-03-19T22:00:49.699129Z","shell.execute_reply":"2024-03-19T22:00:51.790070Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'watermark'...\nremote: Enumerating objects: 80, done.\u001b[K\nremote: Counting objects: 100% (80/80), done.\u001b[K\nremote: Compressing objects: 100% (65/65), done.\u001b[K\nremote: Total 80 (delta 25), reused 67 (delta 14), pack-reused 0\u001b[K\nUnpacking objects: 100% (80/80), 264.81 KiB | 4.20 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/watermark/demo\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:00:56.043075Z","iopub.execute_input":"2024-03-19T22:00:56.043539Z","iopub.status.idle":"2024-03-19T22:00:56.050258Z","shell.execute_reply.started":"2024-03-19T22:00:56.043501Z","shell.execute_reply":"2024-03-19T22:00:56.049062Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os, argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom mersenne import mersenne_rng\n\ndef generate_shift(model,prompt,vocab_size,n,m,key):\n    rng = mersenne_rng(key)\n    xi = torch.tensor([rng.rand() for _ in range(n*vocab_size)]).view(n,vocab_size)\n    shift = torch.randint(n, (1,))\n\n    inputs = prompt.to(model.device)\n    attn = torch.ones_like(inputs)\n    past = None\n    for i in range(m):\n        with torch.no_grad():\n            if past:\n                output = model(inputs[:,-1:], past_key_values=past, attention_mask=attn)\n            else:\n                output = model(inputs)\n\n        probs = torch.nn.functional.softmax(output.logits[:,-1, :vocab_size], dim=-1).cpu()\n        token = exp_sampling(probs,xi[(shift+i)%n,:]).to(model.device)\n        inputs = torch.cat([inputs, token], dim=-1)\n\n        past = output.past_key_values\n        attn = torch.cat([attn, attn.new_ones((attn.shape[0], 1))], dim=-1)\n\n    return inputs.detach().cpu()\n\ndef exp_sampling(probs,u):\n    return torch.argmax(u ** (1/probs),axis=1).unsqueeze(-1)\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model_name = \"facebook/opt-350m\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n    tokens = tokenizer.encode(\"A good thesis\", return_tensors='pt', truncation=True, max_length=2048)\n\n    watermarked_tokens = generate_shift(model,tokens,len(tokenizer),256,80,42)[0]\n    watermarked_text = tokenizer.decode(watermarked_tokens, skip_special_tokens=True)\n\n    print(watermarked_text)\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:00:58.501028Z","iopub.execute_input":"2024-03-19T22:00:58.501582Z","iopub.status.idle":"2024-03-19T22:02:11.496808Z","shell.execute_reply.started":"2024-03-19T22:00:58.501547Z","shell.execute_reply":"2024-03-19T22:02:11.495454Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ac936a108d4b53a28294bd088f0575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6527a55f9af483593f1dcde8b857d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb1d6b1c5eab4511bbb45095f342c47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3169475e8b6246b487278a3356e2c093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b248a144d6491bb5f618b8df28c124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf45850776a409b98b6d902e408c4a4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4243356d2664ceaa6e031f4faf866b8"}},"metadata":{}},{"name":"stdout","text":"A good thesis or even just impress your buddies will clear it up. If you aren't aware of how to properly state your plans, chances are you won't meet anyone\nThere is a whole torrent of stuff on witch Costa products or something and I feel like I haven’t heard of it for a while so wouldn’t it be interesting to be like “ME GET OUT OF HERE WE W\n","output_type":"stream"}]},{"cell_type":"code","source":"import os, sys, argparse, time\n\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom mersenne import mersenne_rng\n\nimport pyximport\npyximport.install(reload_support=True, language_level=sys.version_info[0],\n                  setup_args={'include_dirs':np.get_include()})\nfrom levenshtein import levenshtein\n\ndef permutation_test(tokens,key,n,k,vocab_size,n_runs=100):\n    rng = mersenne_rng(key)\n    xi = np.array([rng.rand() for _ in range(n*vocab_size)], dtype=np.float32).reshape(n,vocab_size)\n    test_result = detect(tokens,n,k,xi)\n\n    p_val = 0\n    for run in range(n_runs):\n        xi_alternative = np.random.rand(n, vocab_size).astype(np.float32)\n        null_result = detect(tokens,n,k,xi_alternative)\n\n        # assuming lower test values indicate presence of watermark\n        p_val += null_result <= test_result\n\n    return (p_val+1.0)/(n_runs+1.0)\n\n\ndef detect(tokens,n,k,xi,gamma=0.0):\n    m = len(tokens)\n    n = len(xi)\n\n    A = np.empty((m-(k-1),n))\n    for i in range(m-(k-1)):\n        for j in range(n):\n            A[i][j] = levenshtein(tokens[i:i+k],xi[(j+np.arange(k))%n],gamma)\n\n    return np.min(A)\n\n\ndef main():\n    text = '''A good thesis or even just impress your buddies will clear it up. If you aren't aware of how to properly state your plans, chances are you won't meet anyone\nThere is a whole torrent of stuff on witch Costa products or something and I feel like I haven’t heard of it for a while so wouldn’t it be interesting to be like “ME GET OUT OF HERE WE W'''\n#     with open(args.document, 'r') as f:\n#         text = f.read()\n\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n    tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=2048).numpy()[0]\n    \n    t0 = time.time()\n    pval = permutation_test(tokens,42,256,len(tokens),len(tokenizer))\n    print('p-value: ', pval)\n    print(f'(elapsed time: {time.time()-t0}s)')\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:02:55.149537Z","iopub.execute_input":"2024-03-19T22:02:55.150186Z","iopub.status.idle":"2024-03-19T22:05:57.617611Z","shell.execute_reply.started":"2024-03-19T22:02:55.150152Z","shell.execute_reply":"2024-03-19T22:05:57.616495Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"In file included from /opt/conda/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h:1929,\n                 from /opt/conda/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n                 from /opt/conda/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h:5,\n                 from /root/.pyxbld/temp.linux-x86_64-cpython-310/kaggle/working/watermark/demo/levenshtein.c:1205:\n/opt/conda/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n      |  ^~~~~~~\n","output_type":"stream"},{"name":"stdout","text":"p-value:  0.009900990099009901\n(elapsed time: 172.32642602920532s)\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    torch.manual_seed(0)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model_name = \"facebook/opt-350m\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n    inputs = tokenizer(\"A good thesis\", return_tensors = \"pt\")\n    generate_ids = model.generate(inputs.input_ids, max_length=2048)\n    answer = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n\n    print(answer)\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:11:13.088022Z","iopub.execute_input":"2024-03-19T22:11:13.088450Z","iopub.status.idle":"2024-03-19T22:11:17.576945Z","shell.execute_reply.started":"2024-03-19T22:11:13.088421Z","shell.execute_reply":"2024-03-19T22:11:17.575726Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"A good thesis is a good thesis.\nI agree. I think it's important to have a good thesis.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os, sys, argparse, time\n\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom mersenne import mersenne_rng\n\nimport pyximport\npyximport.install(reload_support=True, language_level=sys.version_info[0],\n                  setup_args={'include_dirs':np.get_include()})\nfrom levenshtein import levenshtein\n\ndef permutation_test(tokens,key,n,k,vocab_size,n_runs=100):\n    rng = mersenne_rng(key)\n    xi = np.array([rng.rand() for _ in range(n*vocab_size)], dtype=np.float32).reshape(n,vocab_size)\n    test_result = detect(tokens,n,k,xi)\n\n    p_val = 0\n    for run in range(n_runs):\n        xi_alternative = np.random.rand(n, vocab_size).astype(np.float32)\n        null_result = detect(tokens,n,k,xi_alternative)\n\n        # assuming lower test values indicate presence of watermark\n        p_val += null_result <= test_result\n\n    return (p_val+1.0)/(n_runs+1.0)\n\n\ndef detect(tokens,n,k,xi,gamma=0.0):\n    m = len(tokens)\n    n = len(xi)\n\n    A = np.empty((m-(k-1),n))\n    for i in range(m-(k-1)):\n        for j in range(n):\n            A[i][j] = levenshtein(tokens[i:i+k],xi[(j+np.arange(k))%n],gamma)\n\n    return np.min(A)\n\n\ndef main():\n    text = '''A good thesis is a good thesis.\nI agree. I think it's important to have a good thesis.\n'''\n#     with open(args.document, 'r') as f:\n#         text = f.read()\n\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n    tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=2048).numpy()[0]\n    \n    t0 = time.time()\n    pval = permutation_test(tokens,42,256,len(tokens),len(tokenizer))\n    print('p-value: ', pval)\n    print(f'(elapsed time: {time.time()-t0}s)')\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:08:05.179392Z","iopub.execute_input":"2024-03-19T22:08:05.179853Z","iopub.status.idle":"2024-03-19T22:09:23.462582Z","shell.execute_reply.started":"2024-03-19T22:08:05.179821Z","shell.execute_reply":"2024-03-19T22:09:23.460521Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"p-value:  0.24752475247524752\n(elapsed time: 77.93624019622803s)\n","output_type":"stream"}]}]}